{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "407fe4c2",
   "metadata": {},
   "source": [
    "# RAG Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2472bbd7",
   "metadata": {
    "use_thinking": true
   },
   "source": [
    "**RAG (Retrieval-Augmented Generation)** retrieves *relevant information from a knowledge base*, which we then pass to an LLM to generate a response. It's like giving the LLM a *cheat sheet* of *just the right reference material* before asking it to answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bdd2dd",
   "metadata": {},
   "source": [
    "**Why RAG matters**: Long texts can exceed context limits, but more importantly, adding noise makes models lose sight of detail and nuance—despite what needle-in-the-haystack benchmarks claim. RAG lets you supply only what's relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1912191b",
   "metadata": {},
   "source": [
    "**RAG for Category Mapping**\n",
    "\n",
    "Say I want to map a product description against a set of predefined categories from a standardized nomenclature—hundreds of items long. RAG helps by pre-filtering those categories down to just the relevant ones before asking the LLM to pick the best match. This matters because A) it's cheaper, and B) the LLM won't miss the right category when it's not buried in noise. RAG acts as the **smart filter** that gives the LLM a focused shortlist instead of the whole haystack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312d5141",
   "metadata": {
    "time_run": "2025-12-17T14:27:43.761561+00:00"
   },
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"Electronics > Computers > Laptops\",\n",
    "    \"Electronics > Computers > Desktop Computers\",\n",
    "    \"Electronics > Mobile Devices > Smartphones\",\n",
    "    \"Electronics > Mobile Devices > Tablets\",\n",
    "    \"Electronics > Audio > Headphones > Wireless Headphones\",\n",
    "    \"Electronics > Audio > Headphones > Wired Headphones\",\n",
    "    \"Electronics > Audio > Speakers > Bluetooth Speakers\",\n",
    "    \"Home & Kitchen > Furniture > Office Furniture > Desks\",\n",
    "    \"Home & Kitchen > Furniture > Office Furniture > Chairs\",\n",
    "    \"Home & Kitchen > Appliances > Small Appliances > Coffee Makers\",\n",
    "    \"Clothing > Men's Clothing > Shirts\",\n",
    "    \"Clothing > Women's Clothing > Dresses\",\n",
    "    \"Sports & Outdoors > Exercise & Fitness > Yoga > Yoga Mats\",\n",
    "    \"Sports & Outdoors > Exercise & Fitness > Cardio > Treadmills\",\n",
    "    \"Books > Fiction > Science Fiction\",\n",
    "    \"Books > Non-Fiction > Business & Money\",\n",
    "]\n",
    "\n",
    "query = \"Noise-cancelling over-ear bluetooth headphones with 30-hour battery life and premium sound quality\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587782df",
   "metadata": {},
   "source": [
    "Using the answer.ai's reranker library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a309a8c",
   "metadata": {
    "time_run": "2025-12-17T14:27:09.375699+00:00"
   },
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# \n",
    "# pip install -U \"rerankers[transformers]\"==0.10.0\n",
    "# pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d25099",
   "metadata": {},
   "source": [
    "It is important to note that what we are trying to do is **surface a handful or relevent results** that can then be passed to an LLM to do something further (ie. apply a single category for the product description).\n",
    "\n",
    "We can so this because models have been trained (via contrastive learning) to evaluate similarity by evaluating proximity between docs and query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dcf0c8",
   "metadata": {},
   "source": [
    "## Retrieval Architectures Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b5cce2",
   "metadata": {},
   "source": [
    "Once we've decided to retrieve relevant documents, the question becomes: *how do we actually compare a query to documents?* All approaches boil down to encoding text into vectors and measuring similarity — but *when* and *how* we do that encoding matters a lot for both accuracy and speed.\n",
    "\n",
    "Then a quick preview of the three approaches you'll cover:\n",
    "\n",
    "1. **Bi-encoders** — encode separately, compare vectors (fast, scalable)\n",
    "2. **Cross-encoders** — encode together, get direct relevance score (accurate, slow)\n",
    "3. **Late interaction** — encode separately, compare token-by-token (middle ground)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e043db66",
   "metadata": {},
   "source": [
    "## Bi-Encoders\n",
    "\n",
    "Bi-encoders encode the query and each document **separately**, producing a single vector for each. We then compare these vectors using cosine similarity (or dot product) to find the most relevant documents.\n",
    "\n",
    "```\n",
    "Query:    [CLS] \"noise\" \"cancelling\" \"headphones\" [SEP]  →  e_cls_query\n",
    "Document: [CLS] \"Electronics\" \">\" \"Audio\" \">\" ... [SEP]  →  e_cls_doc\n",
    "```\n",
    "> A single vector representation (embedding) is produced for each query and individual document. There are multiple ways to generate this embedding per query/document\n",
    "\n",
    "**Why this matters for scale**: Because documents are encoded independently, we can pre-compute all document embeddings once and store them. At query time, we only encode the query and compare against the stored vectors — this makes bi-encoders very fast.\n",
    "\n",
    "### Pooling Strategies\n",
    "\n",
    "When we fine-tune a model for similarity, it outputs embeddings for the *entire input sequence*. But we need a **single vector** to represent the text. Where does it come from?\n",
    "\n",
    "#### CLS Pooling\n",
    "\n",
    "Use the `[CLS]` token's embedding as the representation. Fine-tuned models are trained to **update the `[CLS]` vector** such that the loss is minimized — despite the model outputting embeddings for the entire sequence, we only use `e_cls`:\n",
    "\n",
    "```\n",
    "Input:  [CLS] \"noise\" \"cancelling\" \"headphones\" [SEP]\n",
    "Output:   ↓      ↓         ↓           ↓         ↓\n",
    "        e_cls   e_1       e_2         e_3       e_sep\n",
    "\n",
    "→ use e_cls (discard the rest)\n",
    "```\n",
    "\n",
    "> The [CLS] embedding is a \"legacy\" implementation from BERT models, where it was originally designed for classification tasks (e.g., sentiment analysis). \n",
    "\n",
    "#### Mean Pooling\n",
    "\n",
    "Average all token embeddings (excluding special tokens):\n",
    "\n",
    "```\n",
    "→ use mean(e_1, e_2, e_3)\n",
    "```\n",
    "\n",
    "**Why mean pooling often works better**: The `[CLS]` token must learn to summarize everything — a lot of pressure on one vector. Mean pooling distributes the representation across all tokens, which can be more robust, especially for longer sequences.\n",
    "\n",
    "> Notice how the `[CLS]` and `[SEP]` embeddings are both dropped\n",
    "\n",
    "**Summary**:\n",
    "```\n",
    "Input:  [CLS] \"noise\" \"cancelling\" \"headphones\" [SEP]\n",
    "Output:   ↓      ↓         ↓           ↓         ↓\n",
    "        e_cls   e_1       e_2         e_3       e_sep\n",
    "\n",
    "CLS pooling:  use e_cls\n",
    "Mean pooling: use mean(e_1, e_2, e_3)\n",
    "```\n",
    "\n",
    "Mean pooling has become the **de facto standard** for bi-encoder models (e.g., `all-MiniLM-L6-v2`, E5, GTE, BGE)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fb789a",
   "metadata": {},
   "source": [
    "### In Practice: Sentence-Transformers\n",
    "\n",
    "The `sentence-transformers` library is the most common way to use bi-encoder models. It wraps the encoding and pooling logic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bd5e23",
   "metadata": {
    "time_run": "2025-12-17T14:28:21.131883+00:00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Noise-cancelling over-ear bluetooth headphones with 30-hour battery life and premium sound quality\n",
      "\n",
      "Score: 0.4245 | Electronics > Audio > Headphones > Wireless Headphones\n",
      "Score: 0.3702 | Electronics > Audio > Headphones > Wired Headphones\n",
      "Score: 0.3460 | Electronics > Audio > Speakers > Bluetooth Speakers\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load a bi-encoder model (mean pooling by default)\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Encode query and documents separately\n",
    "query_embedding = model.encode(query)\n",
    "doc_embeddings = model.encode(docs)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "from sentence_transformers.util import cos_sim\n",
    "\n",
    "similarities = cos_sim(query_embedding, doc_embeddings)[0].numpy()\n",
    "\n",
    "# Get top-k most relevant categories\n",
    "top_k = 3\n",
    "top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "for idx in top_indices:\n",
    "    print(f\"Score: {similarities[idx]:.4f} | {docs[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707b5a59",
   "metadata": {},
   "source": [
    "Pre-computing `doc_embeddings` once, then comparing against new queries, is what makes bi-encoders practical for large-scale retrieval.\n",
    "\n",
    "? Would love to see the **contrastive learning** approach used to train the `[CLS]` or mean pooled embeddings. It make sense at a high-level, but would struggle to go into any detail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852e64b0",
   "metadata": {},
   "source": [
    "## Cross-Encoders\n",
    "\n",
    "Cross-encoders take a fundamentally different approach: instead of encoding query and document separately, they encode them **together** as a single input and output a relevance score directly.\n",
    "\n",
    "```\n",
    "Input:  [CLS] \"noise\" \"cancelling\" \"headphones\" [SEP] \"Electronics\" \">\" \"Audio\" \">\" ... [SEP]\n",
    "                        ↓\n",
    "              Relevance Score: 0.87\n",
    "```\n",
    "\n",
    "> No need to discuss pooling with cross-encoders, since a single relevence score is output by design\n",
    "\n",
    "The model sees both texts at once, allowing full attention between query and document tokens. This means the model can capture fine-grained interactions — like recognizing that \"noise-cancelling\" relates strongly to \"Headphones\" but not to \"Speakers.\"\n",
    "\n",
    "**Why this is more accurate**: Bi-encoders compress each text into a single vector *before* comparison. Cross-encoders delay that compression, letting the model reason about the relationship between query and document directly.\n",
    "\n",
    "**The trade-off**: You can't pre-compute document embeddings. Every query requires running the model on every (query, document) pair. For 1,000 documents, that's 1,000 forward passes per query.\n",
    "\n",
    "Popular cross-encoder models include **ms-marco-MiniLM-L-6-v2** (fast, general-purpose), **ms-marco-electra-base** (stronger but slower), **BGE-reranker** (multilingual), and **Cohere Rerank** (commercial API)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49646ff2",
   "metadata": {},
   "source": [
    "Here we can use the [Reranker lib]() by Answer.AI to use these models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dc1515",
   "metadata": {
    "time_run": "2025-12-17T14:32:19.187647+00:00"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading default cross-encoder model for language en\n",
      "Default Model: mixedbread-ai/mxbai-rerank-base-v1\n",
      "Loading TransformerRanker model mixedbread-ai/mxbai-rerank-base-v1 (this message can be suppressed by setting verbose=0)\n",
      "No device set\n",
      "Using device cpu\n",
      "No dtype set\n",
      "Using dtype torch.float32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model mixedbread-ai/mxbai-rerank-base-v1\n",
      "Using device cpu.\n",
      "Using dtype torch.float32.\n"
     ]
    }
   ],
   "source": [
    "from rerankers import Reranker\n",
    "\n",
    "ranker = Reranker(\n",
    "    'cross-encoder',\n",
    "    model_type='cross-encoder'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7801d48",
   "metadata": {
    "time_run": "2025-12-17T14:32:53.998746+00:00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Noise-cancelling over-ear bluetooth headphones with 30-hour battery life and premium sound quality\n",
      "\n",
      "Top 5 matching categories:\n",
      "Score: -0.7980 - Electronics > Audio > Headphones > Wireless Headphones\n",
      "Score: -0.8287 - Electronics > Audio > Speakers > Bluetooth Speakers\n",
      "Score: -1.3633 - Electronics > Mobile Devices > Smartphones\n",
      "Score: -1.5125 - Electronics > Computers > Laptops\n",
      "Score: -1.5164 - Electronics > Mobile Devices > Tablets\n"
     ]
    }
   ],
   "source": [
    "# Rerank to get top categories\n",
    "results = ranker.rank(query=query, docs=docs, doc_ids=list(range(len(docs))))\n",
    "\n",
    "# Display top 5 results\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Top 5 matching categories:\")\n",
    "for result in results.top_k(5):\n",
    "    print(f\"Score: {result.score:.4f} - {result.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e45436a",
   "metadata": {},
   "source": [
    "## Late Interaction (ColBERT)\n",
    "\n",
    "Late interaction models like ColBERT offer a middle ground: encode query and documents **separately** (like bi-encoders), but compare at the **token level** instead of collapsing to a single vector.\n",
    "\n",
    "```\n",
    "Query:    [CLS] \"noise\" \"cancelling\" \"headphones\" [SEP]  →  [e_q1, e_q2, e_q3]\n",
    "Document: [CLS] \"Electronics\" \">\" \"Audio\" \">\" ... [SEP]  →  [e_d1, e_d2, e_d3, ...]\n",
    "```\n",
    "\n",
    "Each query token finds its best match among the document tokens (MaxSim), and these scores are summed to produce the final relevance score:\n",
    "\n",
    "```\n",
    "Query embeddings:          q₁           q₂            q₃\n",
    "                       \"noise\"   \"cancelling\"   \"headphones\"\n",
    "\n",
    "Document embeddings:   d₁       d₂      d₃       d₄        d₅\n",
    "                     \"Elec\"   \"Audio\"  \">\"    \"Headphones\" \"Wireless\"\n",
    "\n",
    "MaxSim: each query token finds its best match in the document\n",
    "\n",
    "              d₁      d₂      d₃      d₄      d₅\n",
    "            ─────   ─────   ─────   ─────   ─────\n",
    "q₁ \"noise\"   0.2     0.3     0.1     0.4     0.2   →  max = 0.4\n",
    "q₂ \"cancel\"  0.1     0.2     0.1     0.3     0.1   →  max = 0.3\n",
    "q₃ \"headph\"  0.2     0.3     0.1     0.9     0.2   →  max = 0.9\n",
    "                                                      ─────────\n",
    "                                           Score = Σ max = 1.6\n",
    "```\n",
    "\n",
    "This preserves **token-level detail** that bi-encoders lose when they collapse to a single vector.\n",
    "\n",
    "**Why this helps**: A bi-encoder might struggle to distinguish \"wireless headphones\" from \"wired headphones\" because both compress to similar vectors. ColBERT keeps the individual token embeddings, so \"wireless\" can match (or not match) specific document tokens.\n",
    "\n",
    "**The trade-off**: You can still pre-compute document embeddings (good for speed), but you now need to store **all token embeddings** per document, not just one vector. This increases storage significantly.\n",
    "\n",
    "> ColBERT = **Col**contextualized **L**ate Interaction over **BERT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e459d5f",
   "metadata": {
    "time_run": "2025-12-17T14:34:14.234241+00:00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading default colbert model for language en\n",
      "Default Model: colbert-ir/colbertv2.0\n",
      "Loading ColBERTRanker model colbert-ir/colbertv2.0 (this message can be suppressed by setting verbose=0)\n",
      "No device set\n",
      "Using device cpu\n",
      "No dtype set\n",
      "Using dtype torch.float32\n",
      "Loading model colbert-ir/colbertv2.0, this might take a while...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Dim set to: 128 for downcasting\n"
     ]
    }
   ],
   "source": [
    "ranker = Reranker(\"colbert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d638fd0f",
   "metadata": {
    "time_run": "2025-12-17T14:34:32.833689+00:00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Noise-cancelling over-ear bluetooth headphones with 30-hour battery life and premium sound quality\n",
      "\n",
      "Top 5 matching categories:\n",
      "Score: 0.4903 - Electronics > Audio > Headphones > Wireless Headphones\n",
      "Score: 0.4481 - Electronics > Audio > Speakers > Bluetooth Speakers\n",
      "Score: 0.4470 - Electronics > Audio > Headphones > Wired Headphones\n",
      "Score: 0.2709 - Electronics > Mobile Devices > Tablets\n",
      "Score: 0.2650 - Electronics > Mobile Devices > Smartphones\n"
     ]
    }
   ],
   "source": [
    "# Rerank to get top categories\n",
    "results = ranker.rank(query=query, docs=docs)\n",
    "\n",
    "# Display top 5 results\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Top 5 matching categories:\")\n",
    "for result in results.top_k(5):\n",
    "    print(f\"Score: {result.score:.4f} - {result.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e7a0ae",
   "metadata": {},
   "source": [
    "## Score Interpretation in Retrieval Models\n",
    "\n",
    "Below is a summary of the results from our simple test:\n",
    "\n",
    "| Approach | Rank | Result | Score |\n",
    "|----------|------|--------|-------|\n",
    "| **Bi-encoder** (MiniLM) | #1 | Wireless Headphones | 0.4245 |\n",
    "| | #2 | Wired Headphones | 0.3702 |\n",
    "| **Cross-encoder** (mxbai-rerank) | #1 | Wireless Headphones | -0.7980 |\n",
    "| | #2 | Bluetooth Speakers | -0.8287 |\n",
    "| **ColBERT** | #1 | Wireless Headphones | 0.4903 |\n",
    "| | #2 | Bluetooth Speakers | 0.4481 |\n",
    "\n",
    "### Interpreting these results\n",
    "\n",
    "- **Rankings**: Reliable across all model types\n",
    "- **Absolute scores/thresholds**: Only meaningful after empirical calibration on your specific data + model\n",
    "\n",
    "This is why production RAG uses **top-k** (e.g., top 5) rather than score thresholds—thresholds require calibration and break when you change models or domains.\n",
    "\n",
    "> Only trust rankings, not score values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa9793f",
   "metadata": {},
   "source": [
    "## When to Use Which?\n",
    "\n",
    "| Approach | Speed | Accuracy | Pre-computed Embeddings | Best For |\n",
    "|----------|-------|----------|-------------------------|----------|\n",
    "| **Bi-encoder** | ⚡⚡⚡ | Good | 1 vector/doc | First-pass retrieval over large corpora |\n",
    "| **Cross-encoder** | ⚡ | Excellent | None (computed at query time) | Re-ranking a small candidate set |\n",
    "| **ColBERT** | ⚡⚡ | Very Good | N vectors/doc (one per token; potentially storage intensive) | When you need better accuracy than bi-encoders but can't afford full cross-encoder passes |\n",
    "\n",
    "### The Dominant Pattern: Two-Stage Retrieval\n",
    "\n",
    "Often the best setup is **bi-encoder retrieval + cross-encoder reranking + LLM generation**. \n",
    "\n",
    "```\n",
    "Query → Bi-encoder retrieves top-100 → Cross-encoder re-ranks to top-10 → LLM generates answer\n",
    "```\n",
    "\n",
    "> The term \"reranker\" comes from the fact that you're literally **re-ranking** results from a cruder first pass — the bi-encoder does the initial ranking by vector similarity, then the cross-encoder refines that ranking with more accurate scoring.\n",
    "\n",
    "### When ColBERT Makes Sense\n",
    "\n",
    "ColBERT combines the speed of bi-encoders with some of the contextual understanding of cross-encoders, making it suitable for tasks where both speed and precision are crucial.\n",
    "\n",
    "This makes ColBERT suitable for large-scale applications where bi-encoders might miss nuanced matches—for example, a search for \"car maintenance\" could retrieve a document discussing \"automobile care\" by matching tokens at the individual level.\n",
    "\n",
    "The trade-off: you need to store all token embeddings per document, which significantly increases storage requirements.\n"
   ]
  }
 ],
 "metadata": {
  "solveit_dialog_mode": "learning",
  "solveit_ver": 2
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
