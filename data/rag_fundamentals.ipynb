{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "407fe4c2",
   "metadata": {},
   "source": [
    "# RAG Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2472bbd7",
   "metadata": {
    "use_thinking": true
   },
   "source": [
    "**RAG (Retrieval-Augmented Generation)** retrieves *relevant information from a knowledge base*, which we then pass to an LLM to generate a response. It's like giving the LLM a *cheat sheet* of *just the right reference material* before asking it to answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bdd2dd",
   "metadata": {},
   "source": [
    "**Why RAG matters**: Long texts can exceed context limits, but more importantly, adding noise makes models lose sight of detail and nuanceâ€”despite what needle-in-the-haystack benchmarks claim. RAG lets you supply only what's relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1912191b",
   "metadata": {},
   "source": [
    "**RAG for Category Mapping**\n",
    "\n",
    "Say I want to map a product description against a set of predefined categories from a standardized nomenclatureâ€”hundreds of items long. RAG helps by pre-filtering those categories down to just the relevant ones before asking the LLM to pick the best match. This matters because A) it's cheaper, and B) the LLM won't miss the right category when it's not buried in noise. RAG acts as the **smart filter** that gives the LLM a focused shortlist instead of the whole haystack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312d5141",
   "metadata": {
    "time_run": "2025-12-16T11:26:37.550877+00:00"
   },
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"Electronics > Computers > Laptops\",\n",
    "    \"Electronics > Computers > Desktop Computers\",\n",
    "    \"Electronics > Mobile Devices > Smartphones\",\n",
    "    \"Electronics > Mobile Devices > Tablets\",\n",
    "    \"Electronics > Audio > Headphones > Wireless Headphones\",\n",
    "    \"Electronics > Audio > Headphones > Wired Headphones\",\n",
    "    \"Electronics > Audio > Speakers > Bluetooth Speakers\",\n",
    "    \"Home & Kitchen > Furniture > Office Furniture > Desks\",\n",
    "    \"Home & Kitchen > Furniture > Office Furniture > Chairs\",\n",
    "    \"Home & Kitchen > Appliances > Small Appliances > Coffee Makers\",\n",
    "    \"Clothing > Men's Clothing > Shirts\",\n",
    "    \"Clothing > Women's Clothing > Dresses\",\n",
    "    \"Sports & Outdoors > Exercise & Fitness > Yoga > Yoga Mats\",\n",
    "    \"Sports & Outdoors > Exercise & Fitness > Cardio > Treadmills\",\n",
    "    \"Books > Fiction > Science Fiction\",\n",
    "    \"Books > Non-Fiction > Business & Money\",\n",
    "]\n",
    "\n",
    "query = \"Noise-cancelling over-ear bluetooth headphones with 30-hour battery life and premium sound quality\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d25099",
   "metadata": {},
   "source": [
    "It is important to note that what we are trying to do is **surface a handful or relevent results** that can then be passed to an LLM to do something further (ie. apply a single category for the product description)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61951376",
   "metadata": {},
   "source": [
    "## Model Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c46d392",
   "metadata": {},
   "source": [
    "I have discussed elsehwere about BERT embeddings and how they capture contextual meaning of text.\n",
    "\n",
    "At a high level, the models we use for RAG *compare* embeddings for a list of documents (ie. product categories) against the query (ie. product description) to find the most similar documents. In other words, the *meaning determins proximity* between docs and query.\n",
    "\n",
    "However the models we use for RAG are different to BERT base models - they are fine-tuned for *similarity* search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d42e4fa",
   "metadata": {
    "solveit_ai": true,
    "use_thinking": true
   },
   "source": [
    "In a few sentences, how do you train a model for similarity search (assume you would use a BERT base model and then fine tune it? if so, I want to know about the fine tune procedure)\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ \n",
    "\n",
    "Great question! The fine-tuning procedure typically uses **contrastive learning**:\n",
    "\n",
    "1. **Training data**: You need pairs (or triplets) of texts with similarity labels. For example: (query, positive_match) pairs, or (anchor, positive, negative) triplets.\n",
    "\n",
    "2. **Loss function**: The model learns to pull similar pairs *closer together* in embedding space while pushing dissimilar pairs *further apart*. Common losses include:\n",
    "   - **Contrastive loss**: penalizes distance between positives, rewards distance between negatives\n",
    "   - **Triplet loss**: ensures anchor is closer to positive than to negative by some margin\n",
    "   - **Multiple Negatives Ranking Loss**: treats other items in a batch as negatives (very efficient)\n",
    "\n",
    "3. **Architecture tweak**: BERT gives you token-level embeddings, but for similarity you need a single vector per text. So you add a **pooling layer** (e.g., mean of all tokens, or just the [CLS] token) to get one embedding per sentence.\n",
    "\n",
    "The result is a model where `cosine_similarity(embed(text_A), embed(text_B))` meaningfully reflects semantic similarityâ€”unlike base BERT, which wasn't trained with that objective.\n",
    "\n",
    "Does the triplet/contrastive setup make sense, or would an example help clarify?"
   ]
  }
 ],
 "metadata": {
  "solveit_dialog_mode": "learning",
  "solveit_ver": 2
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
