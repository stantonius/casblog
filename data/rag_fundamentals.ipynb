{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "407fe4c2",
   "metadata": {},
   "source": [
    "# RAG Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2472bbd7",
   "metadata": {
    "use_thinking": true
   },
   "source": [
    "**RAG (Retrieval-Augmented Generation)** retrieves *relevant information from a knowledge base*, which we then pass to an LLM to generate a response. It's like giving the LLM a *cheat sheet* of *just the right reference material* before asking it to answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bdd2dd",
   "metadata": {},
   "source": [
    "**Why RAG matters**: Long texts can exceed context limits, but more importantly, adding noise makes models lose sight of detail and nuanceâ€”despite what needle-in-the-haystack benchmarks claim. RAG lets you supply only what's relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1912191b",
   "metadata": {},
   "source": [
    "**RAG for Category Mapping**\n",
    "\n",
    "Say I want to map a product description against a set of predefined categories from a standardized nomenclatureâ€”hundreds of items long. RAG helps by pre-filtering those categories down to just the relevant ones before asking the LLM to pick the best match. This matters because A) it's cheaper, and B) the LLM won't miss the right category when it's not buried in noise. RAG acts as the **smart filter** that gives the LLM a focused shortlist instead of the whole haystack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312d5141",
   "metadata": {
    "time_run": "2025-12-16T11:26:37.550877+00:00"
   },
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"Electronics > Computers > Laptops\",\n",
    "    \"Electronics > Computers > Desktop Computers\",\n",
    "    \"Electronics > Mobile Devices > Smartphones\",\n",
    "    \"Electronics > Mobile Devices > Tablets\",\n",
    "    \"Electronics > Audio > Headphones > Wireless Headphones\",\n",
    "    \"Electronics > Audio > Headphones > Wired Headphones\",\n",
    "    \"Electronics > Audio > Speakers > Bluetooth Speakers\",\n",
    "    \"Home & Kitchen > Furniture > Office Furniture > Desks\",\n",
    "    \"Home & Kitchen > Furniture > Office Furniture > Chairs\",\n",
    "    \"Home & Kitchen > Appliances > Small Appliances > Coffee Makers\",\n",
    "    \"Clothing > Men's Clothing > Shirts\",\n",
    "    \"Clothing > Women's Clothing > Dresses\",\n",
    "    \"Sports & Outdoors > Exercise & Fitness > Yoga > Yoga Mats\",\n",
    "    \"Sports & Outdoors > Exercise & Fitness > Cardio > Treadmills\",\n",
    "    \"Books > Fiction > Science Fiction\",\n",
    "    \"Books > Non-Fiction > Business & Money\",\n",
    "]\n",
    "\n",
    "query = \"Noise-cancelling over-ear bluetooth headphones with 30-hour battery life and premium sound quality\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d25099",
   "metadata": {},
   "source": [
    "It is important to note that what we are trying to do is **surface a handful or relevent results** that can then be passed to an LLM to do something further (ie. apply a single category for the product description)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dcf0c8",
   "metadata": {},
   "source": [
    "## Retrieval Architectures Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b5cce2",
   "metadata": {},
   "source": [
    "Once we've decided to retrieve relevant documents, the question becomes: *how do we actually compare a query to documents?* All approaches boil down to encoding text into vectors and measuring similarity â€” but *when* and *how* we do that encoding matters a lot for both accuracy and speed.\n",
    "\n",
    "Then a quick preview of the three approaches you'll cover:\n",
    "\n",
    "1. **Bi-encoders** â€” encode separately, compare vectors (fast, scalable)\n",
    "2. **Cross-encoders** â€” encode together, get direct relevance score (accurate, slow)\n",
    "3. **Late interaction** â€” encode separately, compare token-by-token (middle ground)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61951376",
   "metadata": {},
   "source": [
    "## Model Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c46d392",
   "metadata": {},
   "source": [
    "I have discussed elsehwere about BERT embeddings and how they capture contextual meaning of text.\n",
    "\n",
    "At a high level, the models we use for RAG *compare* embeddings for a list of documents (ie. product categories) against the query (ie. product description) to find the most similar documents. In other words, the *meaning determins proximity* between docs and query.\n",
    "\n",
    "However the models we use for RAG are different to BERT base models - they are fine-tuned for *similarity* search, typically via **contrastive learning** (pull similar pairs closer together in embedding space while pushing dissimilar pairs further apart)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2217b7e",
   "metadata": {},
   "source": [
    "### Semantic Embedding Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42723cc2",
   "metadata": {
    "use_thinking": true
   },
   "source": [
    "When we fine-tune a model for similarity or RAG, what exactly are they outputting?\n",
    "\n",
    "Traditionally, they output a **single vector representation** for the *entire input sequence*. \n",
    "\n",
    "Where does the single vector come from? There are a few different ways"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cf0faf",
   "metadata": {
    "use_thinking": true
   },
   "source": [
    "#### CLS pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b2a2bd",
   "metadata": {},
   "source": [
    "Utilize the `[CLS]` vector to the output for which we minimize the loss against (already outputted by default with BERT models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfc2343",
   "metadata": {},
   "source": [
    "```\n",
    "[CLS] + query tokens + [SEP] + document tokens + [SEP]\n",
    "                        â†“\n",
    "                   Transformer\n",
    "                        â†“\n",
    "              [CLS] embedding captures\n",
    "           \"how related are these two texts?\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3546cd",
   "metadata": {},
   "source": [
    "The key to note here is fine-tuning models are trained to **update the `[CLS]` vector** such that the loss is minimized. This is despite the fact the model outputs embeddings for *entire input sequence* - we only care about the `[CLS]` embedding vector (`e_cls`) when we calculate the loss\n",
    "\n",
    "```\n",
    "Input:  [CLS] \"noise\" \"cancelling\" \"headphones\" [SEP]\n",
    "Output:   â†“      â†“         â†“           â†“         â†“\n",
    "        e_cls   e_1       e_2         e_3       e_sep\n",
    "\n",
    "```\n",
    "\n",
    "We only care about the `[CLS]` vector, nothing else (discard it)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2730e00b",
   "metadata": {},
   "source": [
    "NOTE: Notice how we are **not** generating the embeddings for the `document` alongside the `query`. These are done **separately** in what is called ***bi-encoder*** approach.\n",
    "\n",
    "With bi-encoders, we do something like this:\n",
    "```\n",
    "Query:    [CLS] \"noise\" \"cancelling\" \"headphones\" [SEP]  â†’  e_cls_query\n",
    "Document: [CLS] \"Electronics\" \">\" \"Audio\" \">\" ... [SEP]  â†’  e_cls_doc\n",
    "```\n",
    "and then compare `e_cls_query` with `e_cls_doc` using cosine similarity (or dot product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6f93a0",
   "metadata": {},
   "source": [
    "Some popular models that use CLS pooling:\n",
    "\n",
    "1. **BERT** (original) - The `[CLS]` token was designed for classification tasks, though raw BERT isn't great for similarity without fine-tuning\n",
    "2. **Sentence-BERT (SBERT)** - One of the first widely-used sentence embedding models, fine-tuned specifically for semantic similarity using CLS pooling (though it also supports mean pooling)\n",
    "4. **ColBERT** (in its global representation mode) - Though ColBERT is famous for its late interaction approach, it can use CLS for certain operations\n",
    "5. **Many cross-encoder models** - When you concatenate query + document and pass through a single encoder, the CLS token captures the relevance score\n",
    "\n",
    "Worth noting: CLS pooling has fallen somewhat out of favor for pure bi-encoder similarity modelsâ€”**mean pooling** (averaging all token embeddings) often works better in practice. But CLS remains very common in cross-encoder setups where query and document are processed together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ed0510",
   "metadata": {
    "use_thinking": true
   },
   "source": [
    "#### Mean Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a711b621",
   "metadata": {
    "skipped": true
   },
   "source": [
    "Mean pooling averages the embeddings of all tokens in the sequence to produce a single vector representation.\n",
    "\n",
    "```\n",
    "Input:  [CLS] \"noise\" \"cancelling\" \"headphones\" [SEP]\n",
    "Output:   â†“      â†“         â†“           â†“         â†“\n",
    "        e_cls   e_1       e_2         e_3       e_sep\n",
    "                 \\         |          /\n",
    "                  \\        |         /\n",
    "                   â†’ mean(e_1, e_2, e_3) â†’ final embedding\n",
    "```\n",
    "\n",
    "Rather than relying on a single special token to carry all the semantic weight, mean pooling lets *every content token **contribute equally** to the final representation*. The `[CLS]` and `[SEP]` tokens are typically excluded from the average (though some implementations include them)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad682466",
   "metadata": {},
   "source": [
    "**Why it often works better than CLS:**\n",
    "- The `[CLS]` token is a single point that must learn to summarize everythingâ€”a lot of pressure on one vector\n",
    "- Mean pooling distributes the representation across all tokens, which can be more robust\n",
    "- For longer sequences especially, averaging captures more of the content\n",
    "\n",
    "**Popular models using mean pooling:**\n",
    "\n",
    "1. **Sentence-Transformers models** (e.g., `all-MiniLM-L6-v2`, `all-mpnet-base-v2`) - the default pooling strategy for most modern sentence embedding models\n",
    "2. **E5 models** (from Microsoft) - use mean pooling with instruction-tuned embeddings\n",
    "3. **GTE models** (from Alibaba) - general text embeddings using mean pooling\n",
    "4. **BGE models** (from BAAI) - another popular family defaulting to mean pooling\n",
    "\n",
    "Mean pooling has become the **de facto standard** for bi-encoder similarity models where query and document are embedded separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a835d9",
   "metadata": {},
   "source": [
    "Again, we would then encode the `document` and comapre the mean of both `query` and `document` embeddings for similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb3df74",
   "metadata": {},
   "source": [
    "> **In summary**:\n",
    "> \n",
    "> ```\n",
    "> Input:  [CLS] \"noise\" \"cancelling\" \"headphones\" [SEP]\n",
    "> Output:   â†“      â†“         â†“           â†“         â†“\n",
    ">         e_cls   e_1       e_2         e_3       e_sep\n",
    ">\n",
    "> CLS pooling:  use e_cls\n",
    "> Mean pooling: use mean(e_1, e_2, e_3)\n",
    "> ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d418e352",
   "metadata": {
    "solveit_ai": true,
    "use_thinking": true
   },
   "source": [
    "fine - so show me the improved blog outline then\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ \n",
    "\n",
    "Here's the restructured outline:\n",
    "\n",
    "```\n",
    "# RAG Fundamentals\n",
    "- What is RAG, why it matters\n",
    "- RAG for Category Mapping (your use case example)\n",
    "\n",
    "## Retrieval Architectures Overview\n",
    "- Brief intro: \"how do we compare query to documents?\"\n",
    "\n",
    "### Bi-Encoders\n",
    "- One sentence: encode query and docs separately, compare vectors\n",
    "- Use sentence-transformers as the concrete example under Bi-Encoders â€” \"Here's how this looks with Sentence-Transformers\" with a quick code snippet showing model.encode(query) and model.encode(docs)\n",
    "- This raises the question: how do we get a single vector?\n",
    "\n",
    "#### Pooling Strategies\n",
    "- CLS pooling (with your token diagram)\n",
    "- Mean pooling\n",
    "- Summary comparison\n",
    "\n",
    "### Cross-Encoders\n",
    "- Encode query + doc together, get relevance score directly\n",
    "- More accurate, but expensive (can't pre-compute doc embeddings)\n",
    "\n",
    "### Late Interaction (ColBERT)\n",
    "- Best of both worlds: embed separately, compare token-by-token\n",
    "- Trade-off: more storage, but better accuracy than bi-encoders\n",
    "\n",
    "## When to Use Which?\n",
    "- Practical guidance / comparison table\n",
    "```\n",
    "\n",
    "The key change: pooling becomes a *subsection* of bi-encoders, since it's specifically answering \"how do bi-encoders get their vectors?\" Cross-encoders don't need pooling discussion (they output a score directly), and ColBERT has its own token-level comparison mechanism.\n",
    "\n",
    "Does this structure feel right?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebacad94",
   "metadata": {
    "solveit_ai": true,
    "use_thinking": true
   },
   "source": [
    "so we have some text under RAG fundamentals. what goes in the Retrieval Architectures overview?\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ \n",
    "\n",
    "This section is your **bridge** â€” it sets up the question that the subsections will answer.\n",
    "\n",
    "Something like:\n",
    "\n",
    "> Once we've decided to retrieve relevant documents, the question becomes: *how do we actually compare a query to documents?* All approaches boil down to encoding text into vectors and measuring similarity â€” but *when* and *how* we do that encoding matters a lot for both accuracy and speed.\n",
    "\n",
    "Then a quick preview of the three approaches you'll cover:\n",
    "\n",
    "1. **Bi-encoders** â€” encode separately, compare vectors (fast, scalable)\n",
    "2. **Cross-encoders** â€” encode together, get direct relevance score (accurate, slow)\n",
    "3. **Late interaction** â€” encode separately, compare token-by-token (middle ground)\n",
    "\n",
    "That's it â€” just enough to orient the reader before diving into bi-encoders.\n",
    "\n",
    "Does that feel like the right level of detail for an overview, or do you want to expand/trim it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cda33de",
   "metadata": {
    "solveit_ai": true,
    "use_thinking": true
   },
   "source": [
    "OK i have added that. what do I put in the next section - bi-encoders? lets re-use as much of my current text as possible (then tell me what we can delete). return the full text that should go in that section\n",
    "\n",
    "##### ðŸ¤–ReplyðŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ \n",
    "\n",
    "Here's the full text for the **Bi-encoders** section:\n",
    "\n",
    "---\n",
    "\n",
    "## Bi-Encoders\n",
    "\n",
    "Bi-encoders encode the query and each document **separately**, producing a single vector for each. We then compare these vectors using cosine similarity (or dot product) to find the most relevant documents.\n",
    "\n",
    "```\n",
    "Query:    [CLS] \"noise\" \"cancelling\" \"headphones\" [SEP]  â†’  e_query\n",
    "Document: [CLS] \"Electronics\" \">\" \"Audio\" \">\" ... [SEP]  â†’  e_doc\n",
    "\n",
    "similarity = cosine(e_query, e_doc)\n",
    "```\n",
    "\n",
    "**Why this matters for scale**: Because documents are encoded independently, we can pre-compute all document embeddings once and store them. At query time, we only encode the query and compare against the stored vectors â€” this makes bi-encoders very fast.\n",
    "\n",
    "### Pooling Strategies\n",
    "\n",
    "When we fine-tune a model for similarity, it outputs embeddings for the *entire input sequence*. But we need a **single vector** to represent the text. Where does it come from?\n",
    "\n",
    "#### CLS Pooling\n",
    "\n",
    "Use the `[CLS]` token's embedding as the representation:\n",
    "\n",
    "```\n",
    "Input:  [CLS] \"noise\" \"cancelling\" \"headphones\" [SEP]\n",
    "Output:   â†“      â†“         â†“           â†“         â†“\n",
    "        e_cls   e_1       e_2         e_3       e_sep\n",
    "\n",
    "â†’ use e_cls\n",
    "```\n",
    "\n",
    "#### Mean Pooling\n",
    "\n",
    "Average all token embeddings (excluding special tokens):\n",
    "\n",
    "```\n",
    "â†’ use mean(e_1, e_2, e_3)\n",
    "```\n",
    "\n",
    "**Why mean pooling often works better**: The `[CLS]` token must learn to summarize everything â€” a lot of pressure on one vector. Mean pooling distributes the representation across all tokens, which can be more robust, especially for longer sequences.\n",
    "\n",
    "Mean pooling has become the **de facto standard** for bi-encoder models (e.g., `all-MiniLM-L6-v2`, E5, GTE, BGE).\n",
    "\n",
    "---\n",
    "\n",
    "**You can delete these existing notes** (they're now absorbed above):\n",
    "- \"Semantic Embedding Models\" \n",
    "- \"CLS pooling\" section\n",
    "- \"Mean Pooling\" section\n",
    "- The summary comparison note\n",
    "- The note starting \"Notice how we are **not** generating...\"\n",
    "\n",
    "Keep the model lists if you want, or I can show you where to work them back in as a reference section later. Want me to continue with Cross-Encoders next?"
   ]
  }
 ],
 "metadata": {
  "solveit_dialog_mode": "learning",
  "solveit_ver": 2
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
