[
  {
    "objectID": "crud.html",
    "href": "crud.html",
    "title": "CRUD",
    "section": "",
    "text": "# fc_tool_info()\nTools available from fastcore.tools:\nfrom fastcore.tools import *\nsource",
    "crumbs": [
      "CRUD"
    ]
  },
  {
    "objectID": "crud.html#add-post",
    "href": "crud.html#add-post",
    "title": "CRUD",
    "section": "Add Post",
    "text": "Add Post\n\nsource\n\nsave_post\n\n save_post (db, post:casblog.core.Post)\n\nSave a Post to the given database, returning new Post with id.\n\nsource\n\n\nparse_categories\n\n parse_categories (s:str)\n\nConvert comma-separated string to JSON array string.",
    "crumbs": [
      "CRUD"
    ]
  },
  {
    "objectID": "crud.html#sync-from-notebook",
    "href": "crud.html#sync-from-notebook",
    "title": "CRUD",
    "section": "Sync from Notebook",
    "text": "Sync from Notebook\nRead content from .ipynb files in /data and update posts in the database. Useful for syncing local notebooks to dev/prod after edits.\n\nsource\n\nread_notebook_markdown\n\n read_notebook_markdown (path, skip_first:bool=False)\n\nRead a Jupyter notebook and extract content as markdown.\n\nMarkdown cells are included as-is\nCode cells are wrapped in ```python blocks\nCells starting with #| directives are skipped\n\nArgs: path: Path to .ipynb file skip_first: If True, skip the first cell (often metadata/links)\n\n# Test read_notebook_markdown\ncontent = read_notebook_markdown(PROJECT_ROOT / \"data\" / \"how_to_read.ipynb\")\nassert \"Filter Framework\" in content, \"Should contain main content\"\nassert \"```python\" not in content or content.count(\"```python\") &gt;= 0, \"Code blocks formatted\"\nprint(f\"Extracted {len(content)} chars, first 200:\\n{content[:200]}...\")\n\n\nsource\n\n\nupdate_post\n\n update_post (db, slug:str, **kwargs)\n\nUpdate an existing post by slug.\nArgs: db: Database connection slug: Post slug to find **kwargs: Fields to update (content, title, categories, etc.)\nReturns: True if post was found and updated, False otherwise\n\n# Test update_post\ndev_db = get_db(prod=False)\nassert update_post(dev_db, 'nonexistent-slug', content='test') == False, \"Should return False for missing post\"\nprint(\"update_post validation passed\")\n\n\nsource\n\n\nsync_notebook_to_post\n\n sync_notebook_to_post (db, slug:str, notebook_path,\n                        skip_first:bool=False)\n\nRead notebook content and update the matching post.\nArgs: db: Database connection slug: Post slug to update notebook_path: Path to .ipynb file skip_first: Skip first cell of notebook\nReturns: True if successful, False if post not found\n\npost_gist = \"https://gist.github.com/stantonius/1124934653f070d345ec4a19c57695f5\"\n\n\ncontent, filename = fetch_gist_markdown(post_gist)\n\ntry:\n    post = Post(\n        title=input(f\"Title [{filename}]: \").strip() or filename.replace('.md', ''),\n        content=content,\n        slug=input(\"Slug (or Enter to auto-generate): \").strip() or None,\n        categories=parse_categories(input(\"Categories (comma-separated): \"))\n    )\nexcept KeyboardInterrupt:\n    print(\"\\nCancelled.\")\n\n\nsaved = save_post(get_db(prod=True), post)",
    "crumbs": [
      "CRUD"
    ]
  },
  {
    "objectID": "crud.html#delete-post",
    "href": "crud.html#delete-post",
    "title": "CRUD",
    "section": "Delete Post",
    "text": "Delete Post\n\ntry:\n    # Fetch all posts from prod\n    prod_db = get_db(prod=True)\n    posts = list(prod_db.t.post.rows)\n\n    # Display posts\n    print(\"Posts in prod database:\\n\")\n    for p in posts:\n        print(f\"  {p['id']}: {p['title']}\")\n\n    # Get selection\n    ids_str = input(\"\\nEnter post IDs to delete (comma-separated), or Enter to cancel: \").strip()\n    if not ids_str:\n        print(\"Cancelled.\")\n    else:\n        ids = [int(i.strip()) for i in ids_str.split(',')]\n        \n        # Show what will be deleted\n        print(f\"\\nâš ï¸  About to delete {len(ids)} post(s):\")\n        for pid in ids:\n            p = next((p for p in posts if p['id'] == pid), None)\n            if p: print(f\"  - {p['title']}\")\n        \n        # Confirm\n        confirm = input(\"\\nType 'DELETE' to confirm: \").strip()\n        if confirm == 'DELETE':\n            for pid in ids:\n                prod_db.t.post.delete(pid)\n                print(f\"ğŸ—‘ï¸  Deleted post id={pid}\")\n            print(\"\\nâœ… Done!\")\n        else:\n            print(\"Cancelled.\")\nexcept KeyboardInterrupt:\n    print(\"\\nCancelled.\")\n\nPosts in prod database:\n\n  2: Token & Embeddings Refresh\n  4: RAG Fundamentals\n  5: How to Read\n\nâš ï¸  About to delete 1 post(s):\n  - RAG Fundamentals\nğŸ—‘ï¸  Deleted post id=4\n\nâœ… Done!",
    "crumbs": [
      "CRUD"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "casblog",
    "section": "",
    "text": "Early stages implementation of my blog using FastHTML within Solveit as an nbdev project\nBecause this is deployed with pla.sh, we had to have the primary file named main.py. I could have just made the primary notebook main.py but that would mean running the plash_deploy --force_data command not in the project root, which complicated where we put the /data directory (used in dev and prod).\nNote: right now we are forcing /data dir update with each deploy via plash_deploy --force_data. This is a temp solution, as this means we are keeping the source of truth prod db within solveit.",
    "crumbs": [
      "casblog"
    ]
  },
  {
    "objectID": "00_core.html",
    "href": "00_core.html",
    "title": "core",
    "section": "",
    "text": "Fill in a module description here\n\n#| default_exp core\n#| hide\nfrom nbdev.showdoc import *\ndocs for fasthtml & jupyter: https://fastht.ml/docs/api/jupyter.html\n\n\n#| exporti\n\nimport re\nfrom collections import Counter\nfrom datetime import datetime\nfrom fasthtml.common import *\nfrom fasthtml.jupyter import *\nfrom fastlite import *\nfrom monsterui.all import *\nfrom urllib.parse import quote, unquote\nfrom fastcore.basics import AttrDict\nfrom fastlucide import *\nimport json\nfrom fastcore.test import *\nfrom pathlib import Path\n#| hide\nfrom dialoghelper import *\n# fc_tool_info()\nTools available from fastcore.tools:\n\n&rg: Run the rg command with the args in argstr (no need to backslash escape)\n&sed: Run the sed command with the args in argstr (e.g for reading a section of a file)\n&view: View directory or file contents with optional line range and numbers\n&create: Creates a new file with the given content at the specified path\n&insert: Insert new_str at specified line number\n&str_replace: Replace first occurrence of old_str with new_str in file\n&strs_replace: Replace for each str pair in old_strs,new_strs\n&replace_lines: Replace lines in file using start and end line-numbers\n\nfrom fastcore.tools import *\n#| export\n#| export\nhdrs = (*Theme.slate.headers(highlightjs=True),\n        Link(rel=\"icon\", href=\"/static/favicon.ico\"),\n        Script(src=\"https://unpkg.com/hyperscript.org@0.9.12\"),\n        Link(rel=\"stylesheet\", href=\"https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:wght@400;600&display=swap\"),\n        Style(\"body { font-family: 'IBM Plex Mono', monospace; } h1,h2,h3,h4,h5,h6 { font-weight: 600; }\")\n)\n\napp = FastHTML(hdrs=hdrs)\n# app.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\nrt = app.route\ndef mk_previewer(app=None, cls=f'max-w-lg'):\n    xcls = cls\n    if not app: app=FastHTML(hdrs=daisy_hdrs)\n    def p(*c, cls='', **kw):\n        return HTMX(Div(cls=f'{xcls} {cls}')(*c), app=app, host=None, port=None, **kw)\n    return p\np = mk_previewer(app)\n\n\n\n#| export\ncfg = AttrDict(\n    name=\"Cas Stantonius\",\n    author=\"Cas\"\n)\n\n\n\n#| export\ndef find_project_root():\n    \"\"\"Walk up from cwd or __file__ to find project root (has settings.ini)\"\"\"\n    # Start from the module file if available, else cwd\n    start = Path(__file__).parent if '__file__' in dir() else Path.cwd()\n    for p in [start, *start.parents]:\n        if (p / 'settings.ini').exists():\n            return p\n    return Path.cwd()  # fallback\n\nPROJECT_ROOT = find_project_root()\n#| export\ndb = database(PROJECT_ROOT / \"data\" / (\"prod.db\" if os.getenv(\"PLASH_PRODUCTION\") else \"dev.db\"))\n\n\n#| export\n@dataclass\nclass Post:\n    title: str\n    content: str\n    slug: str = None\n    created: datetime = None\n    updated: datetime = None\n    categories: str = \"[]\"\n    id: int = None\n    \n    def __post_init__(self):\n        if not self.slug or self.slug is None:\n            self.slug = self._generate_slug(self.title)\n        if not self.created or self.created is None:\n            self.created = datetime.now()\n\n    def _generate_slug(self, title: str):\n        return re.sub(r'[^a-z0-9]+', '-', title.lower()).strip('-')\n    \n    def update(self, regenerate_slug: bool = False, **kwargs):\n        for key, value in kwargs.items():\n            setattr(self, key, value)\n        self.updated = datetime.now()\n        if regenerate_slug:\n            self.slug = self._generate_slug(self.title)\nCategories approach (keeping it simple):\nWeâ€™re storing categories as a JSON string in each post (e.g., '[\"Tech\", \"Life\"]'). On each page load, we:\n\nFetch all posts from the DB\nParse each postâ€™s categories JSON\nFlatten and dedupe to get the full category list\n\nWhy this â€œinefficientâ€ approach?\n\nA personal blog will likely have &lt;100 posts â€” this is trivially fast\nAvoids complexity of caching logic or extra tables\nEasier to reason about and debug\nNo premature optimization â€” optimize when (if) it becomes a problem\n\nFuture optimization options (if needed):\n\nCache categories in memory, refresh on post create/update/delete\nSeparate categories table maintained alongside posts\n\n# Test slug generation\np = Post(title=\"Hello World!\", content=\"test\")\nassert p.slug == \"hello-world\", f\"Expected 'hello-world', got '{p.slug}'\"\n\n# Test special characters in slug\np2 = Post(title=\"What's Up?? @#$ Test\", content=\"test\")\nassert p2.slug == \"what-s-up-test\", f\"Expected 'what-s-up-test', got '{p2.slug}'\"\n\n# Test created timestamp auto-set\np3 = Post(title=\"Timestamp Test\", content=\"test\")\nassert p3.created is not None, \"created should be auto-set\"\nassert isinstance(p3.created, datetime), \"created should be a datetime\"\n\n# Test update method\np4 = Post(title=\"Update Test\", content=\"original\")\np4.update(content=\"modified\", title=\"New Title\")\nassert p4.content == \"modified\", \"content should be updated\"\nassert p4.title == \"New Title\", \"title should be updated\"\nassert p4.updated is not None, \"updated should be set after update()\"\nassert p4.slug == \"update-test\", \"slug should NOT change without regenerate_slug=True\"\n\n# Test slug regeneration\np5 = Post(title=\"Original Title\", content=\"test\")\np5.update(title=\"Brand New Title\", regenerate_slug=True)\nassert p5.slug == \"brand-new-title\", f\"Expected 'brand-new-title', got '{p5.slug}'\"\n\n# Test default categories\np6 = Post(title=\"No Categories\", content=\"test\")\nassert p6.categories == \"[]\", f\"Expected '[]', got '{p6.categories}'\"\n\n# Test categories passed as list (note: your dataclass stores as-is, not auto-JSON)\np7 = Post(title=\"With Categories\", content=\"test\", categories='[\"Tech\", \"Life\"]')\nassert p7.categories == '[\"Tech\", \"Life\"]', \"categories should store the JSON string\"\n\n# Test updating categories\np8 = Post(title=\"Category Update\", content=\"test\")\np8.update(categories='[\"New\", \"Updated\"]')\nassert p8.categories == '[\"New\", \"Updated\"]', \"categories should be updatable\"\n\n# Test explicit slug is NOT overwritten\np_custom_slug = Post(title=\"My Title\", content=\"test\", slug=\"my-custom-slug\")\nassert p_custom_slug.slug == \"my-custom-slug\", \"Explicit slug should be preserved\"\n\n# Test explicit created is NOT overwritten  \ncustom_date = datetime(2020, 1, 15)\np_custom_date = Post(title=\"Old Post\", content=\"test\", created=custom_date)\nassert p_custom_date.created == custom_date, \"Explicit created date should be preserved\"\n\n\nprint(\"Category tests passed! âœ“\")\nCategory tests passed! âœ“\n\n\n\n\n#| export\ndef get_all_categories():\n    \"\"\"Get all unique categories across all posts.\"\"\"\n    posts = db.t.post()\n    all_cats = []\n    for post in posts:\n        all_cats.extend(json.loads(post['categories']))\n    return sorted(set(all_cats))\n\n# Clear any existing test data and insert test posts\ndb.t.post.drop(ignore=True)\ndb.create(Post)\n\n# Insert posts with various category combinations\ndb.t.post.insert(Post(title=\"Post 1\", content=\"test\", categories='[\"Tech\", \"Life\"]'))\ndb.t.post.insert(Post(title=\"Post 2\", content=\"test\", categories='[\"Tech\", \"Travel\"]'))\ndb.t.post.insert(Post(title=\"Post 3\", content=\"test\", categories='[]'))  # empty categories\ndb.t.post.insert(Post(title=\"Post 4\", content=\"test\", categories='[\"Life\"]'))\n\n# Test: should return sorted unique categories\nresult = get_all_categories()\nassert result == [\"Life\", \"Tech\", \"Travel\"], f\"Expected sorted unique cats, got {result}\"\n\n# Test: empty db should return empty list\ndb.t.post.drop(ignore=True)\ndb.create(Post)\nassert get_all_categories() == [], \"Empty db should return empty list\"\n\nprint(\"get_all_categories tests passed! âœ“\")\nget_all_categories tests passed! âœ“\n#| export\ndef get_posts_by_category(category):\n    \"\"\"Get all posts that include the given category.\"\"\"\n    posts = db.t.post()\n    return [p for p in posts if category in json.loads(p['categories'])]\n# Setup test data\ndb.t.post.drop(ignore=True)\ndb.create(Post)\ndb.t.post.insert(Post(title=\"Post 1\", content=\"test\", categories='[\"Tech\", \"Life\"]'))\ndb.t.post.insert(Post(title=\"Post 2\", content=\"test\", categories='[\"Tech\"]'))\ndb.t.post.insert(Post(title=\"Post 3\", content=\"test\", categories='[\"Travel\"]'))\n\n# Test: find posts with \"Tech\" category\nresult = get_posts_by_category(\"Tech\")\nassert len(result) == 2, f\"Expected 2 Tech posts, got {len(result)}\"\n\n# Test: category with no posts\nassert get_posts_by_category(\"Nonexistent\") == [], \"Should return empty list for missing category\"\n\n# Test: single match\nassert len(get_posts_by_category(\"Travel\")) == 1, \"Should find 1 Travel post\"\n\nprint(\"get_posts_by_category tests passed! âœ“\")\nget_posts_by_category tests passed! âœ“\n#| export\ndef format_date(dt: str):\n    return datetime.fromisoformat(dt).strftime('%d %B %Y')\n# Test standard datetime string\nassert format_date(\"2024-03-15T10:30:00\") == \"15 March 2024\"\n\n# Test date-only string\nassert format_date(\"2023-12-01\") == \"01 December 2023\"\n\n# Test with microseconds\nassert format_date(\"2025-01-07T14:22:33.123456\") == \"07 January 2025\"\n\ntest_fail(format_date, args=1234, exc=TypeError)\n\ntest_fail(format_date, args=['not-a-date'], exc=ValueError)\n\nprint(\"format_date tests passed! âœ“\")\nformat_date tests passed! âœ“\nCalling db.t.post() returns dict, not a Post object. Which means we have to convert string datetimes ourselves\n#| export\ndef get_post_by_slug(slug):\n    posts = list(db.t.post.rows_where('slug = ?', [slug], limit=1))\n    return posts[0] if posts else None\n# Setup test data\ndb.t.post.drop(ignore=True)\ndb.create(Post)\ndb.t.post.insert(Post(title=\"Test Post\", content=\"hello\", slug=\"test-post\"))\n\n# Test: find existing post\nresult = get_post_by_slug(\"test-post\")\nassert result is not None\nassert result['title'] == \"Test Post\"\n\n# Test: non-existent slug returns None\nassert get_post_by_slug(\"nope\") is None\n\nprint(\"get_post_by_slug tests passed! âœ“\")\nget_post_by_slug tests passed! âœ“\n\n\n\n#| export\n\nicons = SvgSprites()\n\ndef sidebar():\n    categories = get_all_categories()\n    return Div(\n        # Backdrop overlay - click to close\n        Div(id=\"sidebar-backdrop\", \n            cls=\"fixed inset-0 bg-black/50 z-40 hidden md:hidden\",\n            _=\"on click toggle .translate-x-0 .-translate-x-full on #sidebar then toggle .hidden on me\"),\n        # Sidebar panel\n        Div(\n            Button(icons(\"x\"), cls=\"md:hidden p-2 self-end\", \n                   _=\"on click toggle .translate-x-0 .-translate-x-full on #sidebar then toggle .hidden on #sidebar-backdrop\"),\n            H4(\"Categories\", cls=\"font-semibold mb-2\"),\n            *[A(c, href=f\"/cat/{quote(c)}\", cls=\"block text-sm hover:underline\") for c in categories],\n            cls=\"flex flex-col fixed md:static top-0 left-0 h-full w-48 bg-base-100 p-4 z-50 \"\n                \"transition-transform duration-300 -translate-x-full md:translate-x-0\",\n            id=\"sidebar\"\n        ),\n    )\n#| export\n\ndef navbar():\n    return Div(\n        Div(\n            Button(icons(\"menu\"), cls=\"md:hidden p-2\", \n                   _=\"on click toggle .translate-x-0 .-translate-x-full on #sidebar then toggle .hidden on #sidebar-backdrop\"),\n            A(\"Home\", href=\"/\"), A(\"About\", href=\"/about\"), \n            cls=\"flex gap-4 items-center\"\n        ),\n        A(H3(cfg.name), href='/'),\n        Div(cls=\"w-24\"),\n        cls=\"flex justify-between items-center p-4 border-b\"\n    )\n#| export\n\ndef layout(content):\n    return (\n        Title(cfg.name),  # sets browser tab/page name\n        Div(\n        icons,\n        navbar(),\n        Div(\n            sidebar(),\n            Div(content, cls=\"p-4 w-full\"),\n            cls=\"flex min-h-[80vh]\"\n        ),\n        cls=\"px-4\"\n    )\n    )\n    \n\n\n\n\n#| export\n@rt\ndef index():\n    posts = db.t.post()\n    post_list = Div(\n        *[Card(\n            H3(A(p['title'], href=f\"/post/{p['slug']}\")),\n            P(p['content'][:100], cls=\"text-sm\"),\n            footer=Small(\n                f\"Published {format_date(p['created'])}\" + \n                (f\" Â· Updated {format_date(p['updated'])}\" if p['updated'] else \"\")\n            )\n        ) for i, p in enumerate(posts)],\n        cls=\"space-y-4\"\n    )\n    return layout(post_list)\n#| export\n@rt(\"/cat/{category}\", methods=[\"GET\"])\ndef cat(category: str):\n    category = unquote(category)\n    posts = get_posts_by_category(category)\n    post_list = Div(\n        H2(f\"Category: {category}\", cls=\"text-xl mb-4\"),\n        *[Card(\n            H3(A(p['title'], href=f\"/post/{p['slug']}\")),\n            P(p['content'][:100] + \"...\" if len(p['content']) &gt; 100 else p['content'], cls=\"text-sm\"),\n            footer=Small(\n                f\"Published {format_date(p['created'])}\" + \n                (f\" Â· Updated {format_date(p['updated'])}\" if p['updated'] else \"\")\n            )\n        ) for p in posts],\n        cls=\"space-y-4\"\n    ) if posts else P(f\"No posts in '{category}'\")\n    return layout(post_list)\n#| export\n@rt(\"/post/{slug}\", methods=[\"GET\"])\ndef post(slug: str):\n    \"\"\"\n    Docs for monster ui markdown (`render_md`): https://monsterui.answer.ai/api_ref/docs_markdown\n    \"\"\"\n    post = get_post_by_slug(slug)\n    if not post: return layout(P(\"Post not found\"))\n    return layout(Div(\n        H1(post['title']),\n        Small(\n            f\"Published {format_date(post['created'])}\" + \n            (f\" Â· Updated {format_date(post['updated'])}\" if post['updated'] else \"\")\n        ),\n        Div(render_md(post['content']), cls=\"mt-4\"),\n        cls=\"max-w-2xl\"\n    ))\n#| export\n@rt(\"/about\")\ndef about():\n    return layout(Div(\n        H3(\"About\"),\n        P(f\"Hi, I'm {cfg.author}. Welcome to my blog!\"),\n        cls=\"max-w-2xl\"\n    ))\nI was concerned that using SQLite would be slower than using static files to store blog posts. After thinking about it though, it seems that sqlite is a local file that we are reading. There is no network latency, and because sqlite is a single file, reads are super fast. Therefore there is no real difference in storing static markdown files vs storing their content in the sqlite db.\n\n\n\ndef seed_prod():\n    \"\"\"Seed production db with sample posts.\"\"\"\n    prod_db = database(PROJECT_ROOT / 'data' / \"prod.db\")\n    prod_db.t.post.drop(ignore=True)\n    prod_db.create(Post)\n    \n    sample_posts = [\n        Post(title=\"Welcome to My Blog\", \n             content=\"This is my first post. Excited to share my thoughts here!\", \n             categories='[\"Life\"]'),\n        Post(title=\"Getting Started with FastHTML\", \n             content=\"FastHTML makes building web apps in Python a breeze...\", \n             categories='[\"Tech\", \"Python\"]'),\n        Post(title=\"Weekend Adventures\", \n             content=\"Went hiking last weekend and discovered a beautiful trail.\", \n             categories='[\"Life\", \"Travel\"]'),\n    ]\n    \n    for p in sample_posts:\n        prod_db.t.post.insert(p)\n    \n    print(f\"Seeded {len(sample_posts)} posts to prod.db âœ“\")\n\n# Run it:\n# seed_prod()\n\n\n\n# db.t.post.drop(ignore=True)\n#| notest\nserver = JupyUvi(app)\nGo to Solveit dashboard and click â€˜Copy Public URLâ€™ button to getâ€¦public URL to see any app running (by default port 8000)\n# #| notest\n# server.stop()\n\n\n\nah there is a /data dir in my project root that pla.sh uses\nok so lets say I use the /data folder in my project root to write blog posts. Id want to have a nbs dir within in for ipynb files and also a posts dir which contains the posts that I convert from ipynb to md. does this make sense?\n\n\nğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ \nYes, that makes sense! So your structure would be:\nproject_root/\nâ”œâ”€â”€ data/\nâ”‚   â”œâ”€â”€ dev.db\nâ”‚   â”œâ”€â”€ prod.db\nâ”‚   â”œâ”€â”€ nbs/          # Author notebooks here\nâ”‚   â”‚   â””â”€â”€ my-post.ipynb\nâ”‚   â””â”€â”€ posts/        # Converted markdown\nâ”‚       â””â”€â”€ my-post.md\nThe workflow would be: write in nbs/, convert to posts/, then load from posts/ into the sqlite db.\nA couple of questions to clarify your intent:\n\nWhy both posts/ and sqlite? Are you thinking the md files are the â€œsource of truthâ€ and you periodically sync them into the db? Or is the db the source of truth once imported?\nConversion tool â€” are you planning to use nbdev or jupyter nbconvert to go from ipynb â†’ md?\n\nits a fair question. sqlite allows for quick indexing and grouping based on categories?\n\n\n\nğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ \nExactly right! SQLite gives you:\n\nFast queries â€” filter by category, date ranges, search by title, etc.\nIndexing â€” add indexes on slug, categories, etc. for even faster lookups\nRelationships â€” if you ever want tags, comments, or other linked data\n\nSo the markdown files in posts/ would be your source of truth (easy to version control, edit in any text editor), and the sqlite db is your runtime cache for fast querying.\nThe workflow would be: 1. Write/edit in nbs/ as notebooks 2. Convert to markdown â†’ posts/ 3. Run a sync script that reads posts/*.md and upserts into sqlite\nDoes that match what youâ€™re thinking? And do you want the sync to be manual (run a command) or automatic (detect file changes)?\nis there anything in dialoghelper package that allows you to export to md porgramatically? i ask because you can export an ipynb (the solveit dialog) to md using a button within the solveit interface\n\n\n\nğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ \nGood question! I donâ€™t have visibility into the dialoghelper package contents from here. Let me check whatâ€™s available:\n\n{\n  \"id\": \"toolu_01RxHrjzMSvgKTcJuBMGEsQN\",\n  \"call\": {\n    \"function\": \"rg\",\n    \"arguments\": {\n      \"argstr\": \"-l . --type py dialoghelper\"\n    }\n  },\n  \"result\": \"'dialoghelper: No such file or directory (os error 2)\\\\n'\"\n}\n\n\n{\n  \"id\": \"toolu_01HBGRGasKu1RTjfjsYWiGNX\",\n  \"call\": {\n    \"function\": \"rg\",\n    \"arguments\": {\n      \"argstr\": \"dialoghelper -l\"\n    }\n  },\n  \"result\": \"'00_core.ipynb\\\\n'\"\n}\n\n\n{\n  \"id\": \"toolu_015QBSxbb8bTVvL9iwe8rVRo\",\n  \"call\": {\n    \"function\": \"view\",\n    \"arguments\": {\n      \"path\": \".\"\n    }\n  },\n  \"result\": \"'Directory contents of /app/data/site/nbs:\\\\n/app/data/site/nbs/styles.css\\\\n/app/data/site/nbs/_quart&lt;TRUNCATED&gt;\"\n}\n\nI can see weâ€™re in a nbs folder. Let me try to find where dialoghelper might be installed or defined:\n\n{\n  \"id\": \"toolu_0151yyTpDdgGwj9ut4hi7MNB\",\n  \"call\": {\n    \"function\": \"rg\",\n    \"arguments\": {\n      \"argstr\": \"-g \\\"*.py\\\" \\\"def.*export.*md\\\" /app\"\n    }\n  },\n  \"result\": \"''\"\n}\n\nI donâ€™t have direct access to the dialoghelper package source. Could you run this in a code cell to see whatâ€™s available?\nimport dialoghelper\nprint(dir(dialoghelper))\nThatâ€™ll show us all the functions/classes it exposes, and we can look for anything related to markdown export.\nimport dialoghelper\nprint(dir(dialoghelper))\n['Placements', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', 'add_html', 'add_msg', 'add_scr', 'add_styles', 'ast_grep', 'ast_py', 'call_endp', 'core', 'curr_dialog', 'del_msg', 'dh_settings', 'empty', 'event_get', 'fc_tool_info', 'find_dname', 'find_msg_id', 'find_msgs', 'find_var', 'fire_event', 'gist_file', 'iife', 'import_gist', 'import_string', 'is_usable_tool', 'load_gist', 'md_cls_d', 'mk_toollist', 'msg_idx', 'msg_insert_line', 'msg_replace_lines', 'msg_str_replace', 'msg_strs_replace', 'pop_data', 'read_msg', 'run_msg', 'set_var', 'tool_info', 'update_msg', 'url2note']\nexplore the code\n\n\n\nI donâ€™t see an obvious markdown export function in dialoghelper. The functions I see are mostly for manipulating messages within the dialog (like add_msg, update_msg, read_msg, etc.).\nHowever, since you mentioned thereâ€™s a button in the Solveit interface that exports to markdown, that functionality might be: - Built into the Solveit UI itself (not exposed as a Python API) - Using standard nbformat or nbconvert under the hood\nFor your use case, you could use nbconvert directly to convert notebooks to markdown:\njupyter nbconvert --to markdown my-post.ipynb\nOr programmatically with nbformat + nbconvert libraries (which you already have installed).\nWould you like to explore how to set up a conversion script using those tools?\nok run this and show me what the output of the md file is\n\n\n\nI need a bit more info before I can run it:\n\nDo you have an .ipynb file already that you want me to convert? If so, whatâ€™s the filename/path?\nOr should I create a simple test notebook first, then convert it to show you the output?\n\nWhich would you prefer?\nthe current file is a ipynb file\n\n\n\nGot it! So you want me to convert the current notebook (00_core.ipynb) to markdown and show you the output?\nShould I: 1. Convert it and save the .md file to disk, then show you a preview? 2. Just print out what the markdown would look like without saving?\nWhich would you prefer?\nyes number 1\n\n\n\n\n{\n  \"id\": \"toolu_013fAShbUkTvbJYK3qDqWvfL\",\n  \"call\": {\n    \"function\": \"rg\",\n    \"arguments\": {\n      \"argstr\": \"-l 00_core.ipynb\"\n    }\n  },\n  \"result\": \"'00_core.ipynb\\\\n'\"\n}\n\nHereâ€™s the code to convert the current notebook to markdown:\nimport subprocess\n\nresult = subprocess.run(\n    ['jupyter', 'nbconvert', '--to', 'markdown', '00_core.ipynb'],\n    capture_output=True,\n    text=True\n)\n\nprint(result.stdout)\nprint(result.stderr)\nTry running this and let me know what happens!\nimport subprocess\n\nresult = subprocess.run(\n    ['jupyter', 'nbconvert', '--to', 'markdown', '00_core.ipynb'],\n    capture_output=True,\n    text=True\n)\n\nprint(result.stdout)\nprint(result.stderr)\nImportant to note - they do auto backups so you can restore if something goes haywire: https://docs.pla.sh/how_to/restore_backups.html\n\n\n\n\nUsing https://pla.sh/dashboard\nyou need to create a requirements.txt file based on the above packages. use your tools to get the current packages versions. generate this requirements.txt file in the parent folder of this file\n!pip show python-fasthtml fastlite monsterui fastcore fastlucide | grep -E \"^(Name|Version)\"\nName: python-fasthtml\nVersion: 0.12.36\n\n\nName: fastlite\nVersion: 0.2.2\nName: monsterui\nVersion: 1.0.34\n\n\nName: fastcore\nVersion: 1.8.17\nName: fastlucide\nVersion: 0.0.6\n#| export Instead of running serve here, we created a simple main.py file in the project root because a) pla.sh needs a file called main to run and b) I needed a single place to hold the /data dir\n\n\n\n#| hide\nimport nbdev; nbdev.nbdev_export()"
  },
  {
    "objectID": "00_core.html#setup",
    "href": "00_core.html#setup",
    "title": "core",
    "section": "",
    "text": "#| exporti\n\nimport re\nfrom collections import Counter\nfrom datetime import datetime\nfrom fasthtml.common import *\nfrom fasthtml.jupyter import *\nfrom fastlite import *\nfrom monsterui.all import *\nfrom urllib.parse import quote, unquote\nfrom fastcore.basics import AttrDict\nfrom fastlucide import *\nimport json\nfrom fastcore.test import *\nfrom pathlib import Path\n#| hide\nfrom dialoghelper import *\n# fc_tool_info()\nTools available from fastcore.tools:\n\n&rg: Run the rg command with the args in argstr (no need to backslash escape)\n&sed: Run the sed command with the args in argstr (e.g for reading a section of a file)\n&view: View directory or file contents with optional line range and numbers\n&create: Creates a new file with the given content at the specified path\n&insert: Insert new_str at specified line number\n&str_replace: Replace first occurrence of old_str with new_str in file\n&strs_replace: Replace for each str pair in old_strs,new_strs\n&replace_lines: Replace lines in file using start and end line-numbers\n\nfrom fastcore.tools import *\n#| export\n#| export\nhdrs = (*Theme.slate.headers(highlightjs=True),\n        Link(rel=\"icon\", href=\"/static/favicon.ico\"),\n        Script(src=\"https://unpkg.com/hyperscript.org@0.9.12\"),\n        Link(rel=\"stylesheet\", href=\"https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:wght@400;600&display=swap\"),\n        Style(\"body { font-family: 'IBM Plex Mono', monospace; } h1,h2,h3,h4,h5,h6 { font-weight: 600; }\")\n)\n\napp = FastHTML(hdrs=hdrs)\n# app.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\nrt = app.route\ndef mk_previewer(app=None, cls=f'max-w-lg'):\n    xcls = cls\n    if not app: app=FastHTML(hdrs=daisy_hdrs)\n    def p(*c, cls='', **kw):\n        return HTMX(Div(cls=f'{xcls} {cls}')(*c), app=app, host=None, port=None, **kw)\n    return p\np = mk_previewer(app)"
  },
  {
    "objectID": "00_core.html#config",
    "href": "00_core.html#config",
    "title": "core",
    "section": "",
    "text": "#| export\ncfg = AttrDict(\n    name=\"Cas Stantonius\",\n    author=\"Cas\"\n)"
  },
  {
    "objectID": "00_core.html#database",
    "href": "00_core.html#database",
    "title": "core",
    "section": "",
    "text": "#| export\ndef find_project_root():\n    \"\"\"Walk up from cwd or __file__ to find project root (has settings.ini)\"\"\"\n    # Start from the module file if available, else cwd\n    start = Path(__file__).parent if '__file__' in dir() else Path.cwd()\n    for p in [start, *start.parents]:\n        if (p / 'settings.ini').exists():\n            return p\n    return Path.cwd()  # fallback\n\nPROJECT_ROOT = find_project_root()\n#| export\ndb = database(PROJECT_ROOT / \"data\" / (\"prod.db\" if os.getenv(\"PLASH_PRODUCTION\") else \"dev.db\"))\n\n\n#| export\n@dataclass\nclass Post:\n    title: str\n    content: str\n    slug: str = None\n    created: datetime = None\n    updated: datetime = None\n    categories: str = \"[]\"\n    id: int = None\n    \n    def __post_init__(self):\n        if not self.slug or self.slug is None:\n            self.slug = self._generate_slug(self.title)\n        if not self.created or self.created is None:\n            self.created = datetime.now()\n\n    def _generate_slug(self, title: str):\n        return re.sub(r'[^a-z0-9]+', '-', title.lower()).strip('-')\n    \n    def update(self, regenerate_slug: bool = False, **kwargs):\n        for key, value in kwargs.items():\n            setattr(self, key, value)\n        self.updated = datetime.now()\n        if regenerate_slug:\n            self.slug = self._generate_slug(self.title)\nCategories approach (keeping it simple):\nWeâ€™re storing categories as a JSON string in each post (e.g., '[\"Tech\", \"Life\"]'). On each page load, we:\n\nFetch all posts from the DB\nParse each postâ€™s categories JSON\nFlatten and dedupe to get the full category list\n\nWhy this â€œinefficientâ€ approach?\n\nA personal blog will likely have &lt;100 posts â€” this is trivially fast\nAvoids complexity of caching logic or extra tables\nEasier to reason about and debug\nNo premature optimization â€” optimize when (if) it becomes a problem\n\nFuture optimization options (if needed):\n\nCache categories in memory, refresh on post create/update/delete\nSeparate categories table maintained alongside posts\n\n# Test slug generation\np = Post(title=\"Hello World!\", content=\"test\")\nassert p.slug == \"hello-world\", f\"Expected 'hello-world', got '{p.slug}'\"\n\n# Test special characters in slug\np2 = Post(title=\"What's Up?? @#$ Test\", content=\"test\")\nassert p2.slug == \"what-s-up-test\", f\"Expected 'what-s-up-test', got '{p2.slug}'\"\n\n# Test created timestamp auto-set\np3 = Post(title=\"Timestamp Test\", content=\"test\")\nassert p3.created is not None, \"created should be auto-set\"\nassert isinstance(p3.created, datetime), \"created should be a datetime\"\n\n# Test update method\np4 = Post(title=\"Update Test\", content=\"original\")\np4.update(content=\"modified\", title=\"New Title\")\nassert p4.content == \"modified\", \"content should be updated\"\nassert p4.title == \"New Title\", \"title should be updated\"\nassert p4.updated is not None, \"updated should be set after update()\"\nassert p4.slug == \"update-test\", \"slug should NOT change without regenerate_slug=True\"\n\n# Test slug regeneration\np5 = Post(title=\"Original Title\", content=\"test\")\np5.update(title=\"Brand New Title\", regenerate_slug=True)\nassert p5.slug == \"brand-new-title\", f\"Expected 'brand-new-title', got '{p5.slug}'\"\n\n# Test default categories\np6 = Post(title=\"No Categories\", content=\"test\")\nassert p6.categories == \"[]\", f\"Expected '[]', got '{p6.categories}'\"\n\n# Test categories passed as list (note: your dataclass stores as-is, not auto-JSON)\np7 = Post(title=\"With Categories\", content=\"test\", categories='[\"Tech\", \"Life\"]')\nassert p7.categories == '[\"Tech\", \"Life\"]', \"categories should store the JSON string\"\n\n# Test updating categories\np8 = Post(title=\"Category Update\", content=\"test\")\np8.update(categories='[\"New\", \"Updated\"]')\nassert p8.categories == '[\"New\", \"Updated\"]', \"categories should be updatable\"\n\n# Test explicit slug is NOT overwritten\np_custom_slug = Post(title=\"My Title\", content=\"test\", slug=\"my-custom-slug\")\nassert p_custom_slug.slug == \"my-custom-slug\", \"Explicit slug should be preserved\"\n\n# Test explicit created is NOT overwritten  \ncustom_date = datetime(2020, 1, 15)\np_custom_date = Post(title=\"Old Post\", content=\"test\", created=custom_date)\nassert p_custom_date.created == custom_date, \"Explicit created date should be preserved\"\n\n\nprint(\"Category tests passed! âœ“\")\nCategory tests passed! âœ“"
  },
  {
    "objectID": "00_core.html#helpers",
    "href": "00_core.html#helpers",
    "title": "core",
    "section": "",
    "text": "#| export\ndef get_all_categories():\n    \"\"\"Get all unique categories across all posts.\"\"\"\n    posts = db.t.post()\n    all_cats = []\n    for post in posts:\n        all_cats.extend(json.loads(post['categories']))\n    return sorted(set(all_cats))\n\n# Clear any existing test data and insert test posts\ndb.t.post.drop(ignore=True)\ndb.create(Post)\n\n# Insert posts with various category combinations\ndb.t.post.insert(Post(title=\"Post 1\", content=\"test\", categories='[\"Tech\", \"Life\"]'))\ndb.t.post.insert(Post(title=\"Post 2\", content=\"test\", categories='[\"Tech\", \"Travel\"]'))\ndb.t.post.insert(Post(title=\"Post 3\", content=\"test\", categories='[]'))  # empty categories\ndb.t.post.insert(Post(title=\"Post 4\", content=\"test\", categories='[\"Life\"]'))\n\n# Test: should return sorted unique categories\nresult = get_all_categories()\nassert result == [\"Life\", \"Tech\", \"Travel\"], f\"Expected sorted unique cats, got {result}\"\n\n# Test: empty db should return empty list\ndb.t.post.drop(ignore=True)\ndb.create(Post)\nassert get_all_categories() == [], \"Empty db should return empty list\"\n\nprint(\"get_all_categories tests passed! âœ“\")\nget_all_categories tests passed! âœ“\n#| export\ndef get_posts_by_category(category):\n    \"\"\"Get all posts that include the given category.\"\"\"\n    posts = db.t.post()\n    return [p for p in posts if category in json.loads(p['categories'])]\n# Setup test data\ndb.t.post.drop(ignore=True)\ndb.create(Post)\ndb.t.post.insert(Post(title=\"Post 1\", content=\"test\", categories='[\"Tech\", \"Life\"]'))\ndb.t.post.insert(Post(title=\"Post 2\", content=\"test\", categories='[\"Tech\"]'))\ndb.t.post.insert(Post(title=\"Post 3\", content=\"test\", categories='[\"Travel\"]'))\n\n# Test: find posts with \"Tech\" category\nresult = get_posts_by_category(\"Tech\")\nassert len(result) == 2, f\"Expected 2 Tech posts, got {len(result)}\"\n\n# Test: category with no posts\nassert get_posts_by_category(\"Nonexistent\") == [], \"Should return empty list for missing category\"\n\n# Test: single match\nassert len(get_posts_by_category(\"Travel\")) == 1, \"Should find 1 Travel post\"\n\nprint(\"get_posts_by_category tests passed! âœ“\")\nget_posts_by_category tests passed! âœ“\n#| export\ndef format_date(dt: str):\n    return datetime.fromisoformat(dt).strftime('%d %B %Y')\n# Test standard datetime string\nassert format_date(\"2024-03-15T10:30:00\") == \"15 March 2024\"\n\n# Test date-only string\nassert format_date(\"2023-12-01\") == \"01 December 2023\"\n\n# Test with microseconds\nassert format_date(\"2025-01-07T14:22:33.123456\") == \"07 January 2025\"\n\ntest_fail(format_date, args=1234, exc=TypeError)\n\ntest_fail(format_date, args=['not-a-date'], exc=ValueError)\n\nprint(\"format_date tests passed! âœ“\")\nformat_date tests passed! âœ“\nCalling db.t.post() returns dict, not a Post object. Which means we have to convert string datetimes ourselves\n#| export\ndef get_post_by_slug(slug):\n    posts = list(db.t.post.rows_where('slug = ?', [slug], limit=1))\n    return posts[0] if posts else None\n# Setup test data\ndb.t.post.drop(ignore=True)\ndb.create(Post)\ndb.t.post.insert(Post(title=\"Test Post\", content=\"hello\", slug=\"test-post\"))\n\n# Test: find existing post\nresult = get_post_by_slug(\"test-post\")\nassert result is not None\nassert result['title'] == \"Test Post\"\n\n# Test: non-existent slug returns None\nassert get_post_by_slug(\"nope\") is None\n\nprint(\"get_post_by_slug tests passed! âœ“\")\nget_post_by_slug tests passed! âœ“"
  },
  {
    "objectID": "00_core.html#components",
    "href": "00_core.html#components",
    "title": "core",
    "section": "",
    "text": "#| export\n\nicons = SvgSprites()\n\ndef sidebar():\n    categories = get_all_categories()\n    return Div(\n        # Backdrop overlay - click to close\n        Div(id=\"sidebar-backdrop\", \n            cls=\"fixed inset-0 bg-black/50 z-40 hidden md:hidden\",\n            _=\"on click toggle .translate-x-0 .-translate-x-full on #sidebar then toggle .hidden on me\"),\n        # Sidebar panel\n        Div(\n            Button(icons(\"x\"), cls=\"md:hidden p-2 self-end\", \n                   _=\"on click toggle .translate-x-0 .-translate-x-full on #sidebar then toggle .hidden on #sidebar-backdrop\"),\n            H4(\"Categories\", cls=\"font-semibold mb-2\"),\n            *[A(c, href=f\"/cat/{quote(c)}\", cls=\"block text-sm hover:underline\") for c in categories],\n            cls=\"flex flex-col fixed md:static top-0 left-0 h-full w-48 bg-base-100 p-4 z-50 \"\n                \"transition-transform duration-300 -translate-x-full md:translate-x-0\",\n            id=\"sidebar\"\n        ),\n    )\n#| export\n\ndef navbar():\n    return Div(\n        Div(\n            Button(icons(\"menu\"), cls=\"md:hidden p-2\", \n                   _=\"on click toggle .translate-x-0 .-translate-x-full on #sidebar then toggle .hidden on #sidebar-backdrop\"),\n            A(\"Home\", href=\"/\"), A(\"About\", href=\"/about\"), \n            cls=\"flex gap-4 items-center\"\n        ),\n        A(H3(cfg.name), href='/'),\n        Div(cls=\"w-24\"),\n        cls=\"flex justify-between items-center p-4 border-b\"\n    )\n#| export\n\ndef layout(content):\n    return (\n        Title(cfg.name),  # sets browser tab/page name\n        Div(\n        icons,\n        navbar(),\n        Div(\n            sidebar(),\n            Div(content, cls=\"p-4 w-full\"),\n            cls=\"flex min-h-[80vh]\"\n        ),\n        cls=\"px-4\"\n    )\n    )"
  },
  {
    "objectID": "00_core.html#routes",
    "href": "00_core.html#routes",
    "title": "core",
    "section": "",
    "text": "#| export\n@rt\ndef index():\n    posts = db.t.post()\n    post_list = Div(\n        *[Card(\n            H3(A(p['title'], href=f\"/post/{p['slug']}\")),\n            P(p['content'][:100], cls=\"text-sm\"),\n            footer=Small(\n                f\"Published {format_date(p['created'])}\" + \n                (f\" Â· Updated {format_date(p['updated'])}\" if p['updated'] else \"\")\n            )\n        ) for i, p in enumerate(posts)],\n        cls=\"space-y-4\"\n    )\n    return layout(post_list)\n#| export\n@rt(\"/cat/{category}\", methods=[\"GET\"])\ndef cat(category: str):\n    category = unquote(category)\n    posts = get_posts_by_category(category)\n    post_list = Div(\n        H2(f\"Category: {category}\", cls=\"text-xl mb-4\"),\n        *[Card(\n            H3(A(p['title'], href=f\"/post/{p['slug']}\")),\n            P(p['content'][:100] + \"...\" if len(p['content']) &gt; 100 else p['content'], cls=\"text-sm\"),\n            footer=Small(\n                f\"Published {format_date(p['created'])}\" + \n                (f\" Â· Updated {format_date(p['updated'])}\" if p['updated'] else \"\")\n            )\n        ) for p in posts],\n        cls=\"space-y-4\"\n    ) if posts else P(f\"No posts in '{category}'\")\n    return layout(post_list)\n#| export\n@rt(\"/post/{slug}\", methods=[\"GET\"])\ndef post(slug: str):\n    \"\"\"\n    Docs for monster ui markdown (`render_md`): https://monsterui.answer.ai/api_ref/docs_markdown\n    \"\"\"\n    post = get_post_by_slug(slug)\n    if not post: return layout(P(\"Post not found\"))\n    return layout(Div(\n        H1(post['title']),\n        Small(\n            f\"Published {format_date(post['created'])}\" + \n            (f\" Â· Updated {format_date(post['updated'])}\" if post['updated'] else \"\")\n        ),\n        Div(render_md(post['content']), cls=\"mt-4\"),\n        cls=\"max-w-2xl\"\n    ))\n#| export\n@rt(\"/about\")\ndef about():\n    return layout(Div(\n        H3(\"About\"),\n        P(f\"Hi, I'm {cfg.author}. Welcome to my blog!\"),\n        cls=\"max-w-2xl\"\n    ))\nI was concerned that using SQLite would be slower than using static files to store blog posts. After thinking about it though, it seems that sqlite is a local file that we are reading. There is no network latency, and because sqlite is a single file, reads are super fast. Therefore there is no real difference in storing static markdown files vs storing their content in the sqlite db."
  },
  {
    "objectID": "00_core.html#loading-md-files",
    "href": "00_core.html#loading-md-files",
    "title": "core",
    "section": "",
    "text": "def seed_prod():\n    \"\"\"Seed production db with sample posts.\"\"\"\n    prod_db = database(PROJECT_ROOT / 'data' / \"prod.db\")\n    prod_db.t.post.drop(ignore=True)\n    prod_db.create(Post)\n    \n    sample_posts = [\n        Post(title=\"Welcome to My Blog\", \n             content=\"This is my first post. Excited to share my thoughts here!\", \n             categories='[\"Life\"]'),\n        Post(title=\"Getting Started with FastHTML\", \n             content=\"FastHTML makes building web apps in Python a breeze...\", \n             categories='[\"Tech\", \"Python\"]'),\n        Post(title=\"Weekend Adventures\", \n             content=\"Went hiking last weekend and discovered a beautiful trail.\", \n             categories='[\"Life\", \"Travel\"]'),\n    ]\n    \n    for p in sample_posts:\n        prod_db.t.post.insert(p)\n    \n    print(f\"Seeded {len(sample_posts)} posts to prod.db âœ“\")\n\n# Run it:\n# seed_prod()"
  },
  {
    "objectID": "00_core.html#development",
    "href": "00_core.html#development",
    "title": "core",
    "section": "",
    "text": "# db.t.post.drop(ignore=True)\n#| notest\nserver = JupyUvi(app)\nGo to Solveit dashboard and click â€˜Copy Public URLâ€™ button to getâ€¦public URL to see any app running (by default port 8000)\n# #| notest\n# server.stop()"
  },
  {
    "objectID": "00_core.html#usage",
    "href": "00_core.html#usage",
    "title": "core",
    "section": "",
    "text": "ah there is a /data dir in my project root that pla.sh uses\nok so lets say I use the /data folder in my project root to write blog posts. Id want to have a nbs dir within in for ipynb files and also a posts dir which contains the posts that I convert from ipynb to md. does this make sense?\n\n\nğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ \nYes, that makes sense! So your structure would be:\nproject_root/\nâ”œâ”€â”€ data/\nâ”‚   â”œâ”€â”€ dev.db\nâ”‚   â”œâ”€â”€ prod.db\nâ”‚   â”œâ”€â”€ nbs/          # Author notebooks here\nâ”‚   â”‚   â””â”€â”€ my-post.ipynb\nâ”‚   â””â”€â”€ posts/        # Converted markdown\nâ”‚       â””â”€â”€ my-post.md\nThe workflow would be: write in nbs/, convert to posts/, then load from posts/ into the sqlite db.\nA couple of questions to clarify your intent:\n\nWhy both posts/ and sqlite? Are you thinking the md files are the â€œsource of truthâ€ and you periodically sync them into the db? Or is the db the source of truth once imported?\nConversion tool â€” are you planning to use nbdev or jupyter nbconvert to go from ipynb â†’ md?\n\nits a fair question. sqlite allows for quick indexing and grouping based on categories?\n\n\n\nğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ \nExactly right! SQLite gives you:\n\nFast queries â€” filter by category, date ranges, search by title, etc.\nIndexing â€” add indexes on slug, categories, etc. for even faster lookups\nRelationships â€” if you ever want tags, comments, or other linked data\n\nSo the markdown files in posts/ would be your source of truth (easy to version control, edit in any text editor), and the sqlite db is your runtime cache for fast querying.\nThe workflow would be: 1. Write/edit in nbs/ as notebooks 2. Convert to markdown â†’ posts/ 3. Run a sync script that reads posts/*.md and upserts into sqlite\nDoes that match what youâ€™re thinking? And do you want the sync to be manual (run a command) or automatic (detect file changes)?\nis there anything in dialoghelper package that allows you to export to md porgramatically? i ask because you can export an ipynb (the solveit dialog) to md using a button within the solveit interface\n\n\n\nğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ \nGood question! I donâ€™t have visibility into the dialoghelper package contents from here. Let me check whatâ€™s available:\n\n{\n  \"id\": \"toolu_01RxHrjzMSvgKTcJuBMGEsQN\",\n  \"call\": {\n    \"function\": \"rg\",\n    \"arguments\": {\n      \"argstr\": \"-l . --type py dialoghelper\"\n    }\n  },\n  \"result\": \"'dialoghelper: No such file or directory (os error 2)\\\\n'\"\n}\n\n\n{\n  \"id\": \"toolu_01HBGRGasKu1RTjfjsYWiGNX\",\n  \"call\": {\n    \"function\": \"rg\",\n    \"arguments\": {\n      \"argstr\": \"dialoghelper -l\"\n    }\n  },\n  \"result\": \"'00_core.ipynb\\\\n'\"\n}\n\n\n{\n  \"id\": \"toolu_015QBSxbb8bTVvL9iwe8rVRo\",\n  \"call\": {\n    \"function\": \"view\",\n    \"arguments\": {\n      \"path\": \".\"\n    }\n  },\n  \"result\": \"'Directory contents of /app/data/site/nbs:\\\\n/app/data/site/nbs/styles.css\\\\n/app/data/site/nbs/_quart&lt;TRUNCATED&gt;\"\n}\n\nI can see weâ€™re in a nbs folder. Let me try to find where dialoghelper might be installed or defined:\n\n{\n  \"id\": \"toolu_0151yyTpDdgGwj9ut4hi7MNB\",\n  \"call\": {\n    \"function\": \"rg\",\n    \"arguments\": {\n      \"argstr\": \"-g \\\"*.py\\\" \\\"def.*export.*md\\\" /app\"\n    }\n  },\n  \"result\": \"''\"\n}\n\nI donâ€™t have direct access to the dialoghelper package source. Could you run this in a code cell to see whatâ€™s available?\nimport dialoghelper\nprint(dir(dialoghelper))\nThatâ€™ll show us all the functions/classes it exposes, and we can look for anything related to markdown export.\nimport dialoghelper\nprint(dir(dialoghelper))\n['Placements', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', 'add_html', 'add_msg', 'add_scr', 'add_styles', 'ast_grep', 'ast_py', 'call_endp', 'core', 'curr_dialog', 'del_msg', 'dh_settings', 'empty', 'event_get', 'fc_tool_info', 'find_dname', 'find_msg_id', 'find_msgs', 'find_var', 'fire_event', 'gist_file', 'iife', 'import_gist', 'import_string', 'is_usable_tool', 'load_gist', 'md_cls_d', 'mk_toollist', 'msg_idx', 'msg_insert_line', 'msg_replace_lines', 'msg_str_replace', 'msg_strs_replace', 'pop_data', 'read_msg', 'run_msg', 'set_var', 'tool_info', 'update_msg', 'url2note']\nexplore the code\n\n\n\nI donâ€™t see an obvious markdown export function in dialoghelper. The functions I see are mostly for manipulating messages within the dialog (like add_msg, update_msg, read_msg, etc.).\nHowever, since you mentioned thereâ€™s a button in the Solveit interface that exports to markdown, that functionality might be: - Built into the Solveit UI itself (not exposed as a Python API) - Using standard nbformat or nbconvert under the hood\nFor your use case, you could use nbconvert directly to convert notebooks to markdown:\njupyter nbconvert --to markdown my-post.ipynb\nOr programmatically with nbformat + nbconvert libraries (which you already have installed).\nWould you like to explore how to set up a conversion script using those tools?\nok run this and show me what the output of the md file is\n\n\n\nI need a bit more info before I can run it:\n\nDo you have an .ipynb file already that you want me to convert? If so, whatâ€™s the filename/path?\nOr should I create a simple test notebook first, then convert it to show you the output?\n\nWhich would you prefer?\nthe current file is a ipynb file\n\n\n\nGot it! So you want me to convert the current notebook (00_core.ipynb) to markdown and show you the output?\nShould I: 1. Convert it and save the .md file to disk, then show you a preview? 2. Just print out what the markdown would look like without saving?\nWhich would you prefer?\nyes number 1\n\n\n\n\n{\n  \"id\": \"toolu_013fAShbUkTvbJYK3qDqWvfL\",\n  \"call\": {\n    \"function\": \"rg\",\n    \"arguments\": {\n      \"argstr\": \"-l 00_core.ipynb\"\n    }\n  },\n  \"result\": \"'00_core.ipynb\\\\n'\"\n}\n\nHereâ€™s the code to convert the current notebook to markdown:\nimport subprocess\n\nresult = subprocess.run(\n    ['jupyter', 'nbconvert', '--to', 'markdown', '00_core.ipynb'],\n    capture_output=True,\n    text=True\n)\n\nprint(result.stdout)\nprint(result.stderr)\nTry running this and let me know what happens!\nimport subprocess\n\nresult = subprocess.run(\n    ['jupyter', 'nbconvert', '--to', 'markdown', '00_core.ipynb'],\n    capture_output=True,\n    text=True\n)\n\nprint(result.stdout)\nprint(result.stderr)\nImportant to note - they do auto backups so you can restore if something goes haywire: https://docs.pla.sh/how_to/restore_backups.html"
  },
  {
    "objectID": "00_core.html#deploy",
    "href": "00_core.html#deploy",
    "title": "core",
    "section": "",
    "text": "Using https://pla.sh/dashboard\nyou need to create a requirements.txt file based on the above packages. use your tools to get the current packages versions. generate this requirements.txt file in the parent folder of this file\n!pip show python-fasthtml fastlite monsterui fastcore fastlucide | grep -E \"^(Name|Version)\"\nName: python-fasthtml\nVersion: 0.12.36\n\n\nName: fastlite\nVersion: 0.2.2\nName: monsterui\nVersion: 1.0.34\n\n\nName: fastcore\nVersion: 1.8.17\nName: fastlucide\nVersion: 0.0.6\n#| export Instead of running serve here, we created a simple main.py file in the project root because a) pla.sh needs a file called main to run and b) I needed a single place to hold the /data dir"
  },
  {
    "objectID": "00_core.html#export",
    "href": "00_core.html#export",
    "title": "core",
    "section": "",
    "text": "#| hide\nimport nbdev; nbdev.nbdev_export()"
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "docs for fasthtml & jupyter: https://fastht.ml/docs/api/jupyter.html",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "core.html#setup",
    "href": "core.html#setup",
    "title": "core",
    "section": "Setup",
    "text": "Setup\n\n# fc_tool_info()\n\nTools available from fastcore.tools:\n\n&rg: Run the rg command with the args in argstr (no need to backslash escape)\n&sed: Run the sed command with the args in argstr (e.g for reading a section of a file)\n&view: View directory or file contents with optional line range and numbers\n&create: Creates a new file with the given content at the specified path\n&insert: Insert new_str at specified line number\n&str_replace: Replace first occurrence of old_str with new_str in file\n&strs_replace: Replace for each str pair in old_strs,new_strs\n&replace_lines: Replace lines in file using start and end line-numbers\n\n\nfrom fastcore.tools import *\n\n\nsource\n\nfind_project_root\n\n find_project_root ()\n\nWalk up from cwd or file to find project root (has settings.ini)",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "core.html#config",
    "href": "core.html#config",
    "title": "core",
    "section": "Config",
    "text": "Config",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "core.html#database",
    "href": "core.html#database",
    "title": "core",
    "section": "Database",
    "text": "Database\ndoes the above check if plash_production exists or is truthy?\n\nğŸ¤–ReplyğŸ¤–\nğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ \nIt checks if the environment variable exists and is non-empty â€” but it doesnâ€™t check if the value is specifically \"true\" or \"1\".\nos.getenv(\"PLASH_PRODUCTION\") returns: - None if the env var doesnâ€™t exist - The string value if it does exist (even if itâ€™s \"false\" or \"0\")\nSo setting PLASH_PRODUCTION=false would actually be truthy and use prod.db!\nIf you want to check for a specific truthy value, youâ€™d do something like:\nos.getenv(\"PLASH_PRODUCTION\", \"\").lower() in (\"1\", \"true\", \"yes\")\nIs that the behavior you intended, or do you want to tighten it up?\n\n\nDataclasses\n\nsource\n\n\nPost\n\n Post (title:str, content:str, slug:str=None,\n       created:datetime.datetime=None, updated:datetime.datetime=None,\n       categories:str='[]', id:int=None)\n\nCategories approach (keeping it simple):\nWeâ€™re storing categories as a JSON string in each post (e.g., '[\"Tech\", \"Life\"]'). On each page load, we:\n\nFetch all posts from the DB\nParse each postâ€™s categories JSON\nFlatten and dedupe to get the full category list\n\nWhy this â€œinefficientâ€ approach?\n\nA personal blog will likely have &lt;100 posts â€” this is trivially fast\nAvoids complexity of caching logic or extra tables\nEasier to reason about and debug\nNo premature optimization â€” optimize when (if) it becomes a problem\n\nFuture optimization options (if needed):\n\nCache categories in memory, refresh on post create/update/delete\nSeparate categories table maintained alongside posts\n\n\n# Test slug generation\np = Post(title=\"Hello World!\", content=\"test\")\nassert p.slug == \"hello-world\", f\"Expected 'hello-world', got '{p.slug}'\"\n\n# Test special characters in slug\np2 = Post(title=\"What's Up?? @#$ Test\", content=\"test\")\nassert p2.slug == \"what-s-up-test\", f\"Expected 'what-s-up-test', got '{p2.slug}'\"\n\n# Test created timestamp auto-set\np3 = Post(title=\"Timestamp Test\", content=\"test\")\nassert p3.created is not None, \"created should be auto-set\"\nassert isinstance(p3.created, datetime), \"created should be a datetime\"\n\n# Test update method\np4 = Post(title=\"Update Test\", content=\"original\")\np4.update(content=\"modified\", title=\"New Title\")\nassert p4.content == \"modified\", \"content should be updated\"\nassert p4.title == \"New Title\", \"title should be updated\"\nassert p4.updated is not None, \"updated should be set after update()\"\nassert p4.slug == \"update-test\", \"slug should NOT change without regenerate_slug=True\"\n\n# Test slug regeneration\np5 = Post(title=\"Original Title\", content=\"test\")\np5.update(title=\"Brand New Title\", regenerate_slug=True)\nassert p5.slug == \"brand-new-title\", f\"Expected 'brand-new-title', got '{p5.slug}'\"\n\n# Test default categories\np6 = Post(title=\"No Categories\", content=\"test\")\nassert p6.categories == \"[]\", f\"Expected '[]', got '{p6.categories}'\"\n\n# Test categories passed as list (note: your dataclass stores as-is, not auto-JSON)\np7 = Post(title=\"With Categories\", content=\"test\", categories='[\"Tech\", \"Life\"]')\nassert p7.categories == '[\"Tech\", \"Life\"]', \"categories should store the JSON string\"\n\n# Test updating categories\np8 = Post(title=\"Category Update\", content=\"test\")\np8.update(categories='[\"New\", \"Updated\"]')\nassert p8.categories == '[\"New\", \"Updated\"]', \"categories should be updatable\"\n\n# Test explicit slug is NOT overwritten\np_custom_slug = Post(title=\"My Title\", content=\"test\", slug=\"my-custom-slug\")\nassert p_custom_slug.slug == \"my-custom-slug\", \"Explicit slug should be preserved\"\n\n# Test explicit created is NOT overwritten  \ncustom_date = datetime(2020, 1, 15)\np_custom_date = Post(title=\"Old Post\", content=\"test\", created=custom_date)\nassert p_custom_date.created == custom_date, \"Explicit created date should be preserved\"\n\n\nprint(\"Category tests passed! âœ“\")\n\nCategory tests passed! âœ“",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "core.html#helpers",
    "href": "core.html#helpers",
    "title": "core",
    "section": "Helpers",
    "text": "Helpers\n\nsource\n\nget_all_categories\n\n get_all_categories ()\n\nGet all unique categories across all posts.\n\n# Use isolated test database (don't touch dev.db)\n_original_db = db\ndb = database(\":memory:\")\ndb.create(Post)\n\n# Insert posts with various category combinations\ndb.t.post.insert(Post(title=\"Post 1\", content=\"test\", categories='[\"Tech\", \"Life\"]'))\ndb.t.post.insert(Post(title=\"Post 2\", content=\"test\", categories='[\"Tech\", \"Travel\"]'))\ndb.t.post.insert(Post(title=\"Post 3\", content=\"test\", categories='[]'))  # empty categories\ndb.t.post.insert(Post(title=\"Post 4\", content=\"test\", categories='[\"Life\"]'))\n\n# Test: should return sorted unique categories\nresult = get_all_categories()\nassert result == [\"Life\", \"Tech\", \"Travel\"], f\"Expected sorted unique cats, got {result}\"\n\n# Test: empty db should return empty list\ndb.t.post.drop(ignore=True)\ndb.create(Post)\nassert get_all_categories() == [], \"Empty db should return empty list\"\n\n# Restore original db\ndb = _original_db\n\nprint(\"get_all_categories tests passed! âœ“\")\n\n\nsource\n\n\nget_posts_by_category\n\n get_posts_by_category (category)\n\nGet all posts that include the given category.\n\n# Use isolated test database (don't touch dev.db)\n_original_db = db\ndb = database(\":memory:\")\ndb.create(Post)\n\ndb.t.post.insert(Post(title=\"Post 1\", content=\"test\", categories='[\"Tech\", \"Life\"]'))\ndb.t.post.insert(Post(title=\"Post 2\", content=\"test\", categories='[\"Tech\"]'))\ndb.t.post.insert(Post(title=\"Post 3\", content=\"test\", categories='[\"Travel\"]'))\n\n# Test: find posts with \"Tech\" category\nresult = get_posts_by_category(\"Tech\")\nassert len(result) == 2, f\"Expected 2 Tech posts, got {len(result)}\"\n\n# Test: category with no posts\nassert get_posts_by_category(\"Nonexistent\") == [], \"Should return empty list for missing category\"\n\n# Test: single match\nassert len(get_posts_by_category(\"Travel\")) == 1, \"Should find 1 Travel post\"\n\n# Restore original db\ndb = _original_db\n\nprint(\"get_posts_by_category tests passed! âœ“\")\n\n\nsource\n\n\nformat_date\n\n format_date (dt:str)\n\n\n# Test standard datetime string\nassert format_date(\"2024-03-15T10:30:00\") == \"15 March 2024\"\n\n# Test date-only string\nassert format_date(\"2023-12-01\") == \"01 December 2023\"\n\n# Test with microseconds\nassert format_date(\"2025-01-07T14:22:33.123456\") == \"07 January 2025\"\n\ntest_fail(format_date, args=1234, exc=TypeError)\n\ntest_fail(format_date, args=['not-a-date'], exc=ValueError)\n\nprint(\"format_date tests passed! âœ“\")\n\nformat_date tests passed! âœ“\n\n\nCalling db.t.post() returns dict, not a Post object. Which means we have to convert string datetimes ourselves\n\nsource\n\n\nget_post_by_slug\n\n get_post_by_slug (slug)\n\n\n# Use isolated test database (don't touch dev.db)\n_original_db = db\ndb = database(\":memory:\")\ndb.create(Post)\n\ndb.t.post.insert(Post(title=\"Test Post\", content=\"hello\", slug=\"test-post\"))\n\n# Test: find existing post\nresult = get_post_by_slug(\"test-post\")\nassert result is not None\nassert result['title'] == \"Test Post\"\n\n# Test: non-existent slug returns None\nassert get_post_by_slug(\"nope\") is None\n\n# Restore original db\ndb = _original_db\n\nprint(\"get_post_by_slug tests passed! âœ“\")\n\n\nToC Nav Helpers\n\nsource\n\n\n\nextract_headers\n\n extract_headers (md_content)\n\nExtract headers from markdown, return list of (level, text, slug).\n\nsource\n\n\nrender_md_with_ids\n\n render_md_with_ids (md_content)\n\nRender markdown and add IDs to headers for scrollspy.",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "core.html#components",
    "href": "core.html#components",
    "title": "core",
    "section": "Components",
    "text": "Components\n\nsource\n\nsidebar\n\n sidebar ()\n\n\nsource\n\n\ntheme_toggle\n\n theme_toggle ()\n\nSingle button theme toggle - shows sun in dark mode, moon in light mode.\n\nsource\n\n\nnavbar\n\n navbar ()\n\n\nsource\n\n\nlayout\n\n layout (content, show_sidebar=True)\n\n\nsource\n\n\ntoc_nav\n\n toc_nav (headers)",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "core.html#routes",
    "href": "core.html#routes",
    "title": "core",
    "section": "Routes",
    "text": "Routes\n\nsource\n\nindex\n\n index ()\n\n\nsource\n\n\ncat\n\n cat (category:str)\n\n\nsource\n\n\npost\n\n post (slug:str)\n\n\nsource\n\n\nabout\n\n about ()\n\nvisit the blog in prod: https://cas-stantonius-site.pla.sh/post/how-to-read#reply-4\ndo you see all of the â€˜ğŸ¤–ReplyğŸ¤–â€™ that are converted to headers in the ToC? why does the parser keep these in?\n\nğŸ¤–ReplyğŸ¤–\nğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ \n\n{\n  \"id\": \"toolu_01PJ4rvwHVroLXSeRuRHLaKn\",\n  \"call\": {\n    \"function\": \"read_url\",\n    \"arguments\": {\n      \"url\": \"https://cas-stantonius-site.pla.sh/post/how-to-read\"\n    }\n  },\n  \"result\": \"'[Home](/)[About](/about)\\\\n\\\\n### [Cas Stantonius ](/)\\\\n\\\\n#### Categories\\\\n\\\\n[LLMs](/cat/LLMs)[Personal Development](/cat/Personal%20Development)\\\\n\\\\n# How to Read\\\\n\\\\nPublished 14 December 2025\\\\n\\\\n# How to Read\\\\n\\\\nResources:\\\\n\\\\n  * [Farnham Street\\\\'s summary of How to Read a Book](https://fs.blog/how-to-read-a-book/)\\\\n\\\\n\\\\n\\\\nI have a problem. I feel like I have to memorize non-fiction books, which causes me to either read slow or avoid reading altogether out of fear of what I am missing.\\\\n\\\\nMy guess is this stems from seeing \\\\'photographic memory\\\\' folks read something once and be able to recite &lt;TRUNCATED&gt;\"\n}\n\nI can see the issue clearly. Looking at the ToC on the page, there are many ğŸ¤–ReplyğŸ¤– entries showing up as headers.\nThe problem is in your extract_headers function. Let me check the filtering logic:\n# Skip headers that are mostly emoji/special chars (no alphanumeric content)\nif not re.search(r'[a-zA-Z0-9]', text_clean):\n    continue\nThe filter checks if thereâ€™s any alphanumeric content. But ğŸ¤–ReplyğŸ¤– contains the word â€œReplyâ€ â€” which does have alphanumeric characters (R, e, p, l, y). So it passes the filter!\nThe regex [a-zA-Z0-9] matches â€œReplyâ€, so the header is kept.\nOptions to fix:\n\nExplicitly skip headers matching a pattern â€” e.g., skip any header thatâ€™s just ğŸ¤–ReplyğŸ¤–\nRequire a minimum amount of alphanumeric content â€” e.g., â€œmust be &gt;50% alphanumericâ€\nUse a blocklist â€” skip headers containing certain phrases like â€œReplyâ€ surrounded by emoji\n\nWhich approach feels right for your use case? Do you want to filter out all ğŸ¤–ReplyğŸ¤– headers, or is there a more general rule youâ€™d prefer?\nyes I want all of these out always\n\n\nğŸ¤–ReplyğŸ¤–\nğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ \nGot it â€” you want to filter out all the ğŸ¤–ReplyğŸ¤– headers from the ToC.\nThe simplest fix is to add a check that skips headers matching that pattern. Hereâ€™s the line to add right after the text_clean processing:\n# Skip reply markers\nif re.search(r'ğŸ¤–.*Reply.*ğŸ¤–', text):\n    continue\nAdd this inside extract_headers, just before the base_slug line. Youâ€™ll also need the same check in render_md_with_ids if you want consistency (though for the ToC, only extract_headers matters).\nWant me to show the full updated function, or do you want to try adding it yourself?\nshow me the full updated function\n\n\nğŸ¤–ReplyğŸ¤–\nğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ \nHereâ€™s the updated extract_headers function:\ndef extract_headers(md_content):\n    \"\"\"Extract headers from markdown, return list of (level, text, slug).\"\"\"\n    headers = []\n    slug_counts = Counter()\n    in_code_block = False\n    for line in md_content.split('\\n'):\n        if line.strip().startswith('```') or line.strip().startswith('~~~'):\n            in_code_block = not in_code_block\n            continue\n        if not in_code_block and (m := re.match(r'^(#{1,6})\\s+(.+)$', line)):\n            level, text = len(m.group(1)), m.group(2).strip()\n            # Strip HTML comments (both encoded and raw forms)\n            text_clean = re.sub(r'&lt;!--.*?--&gt;', '', text)\n            text_clean = re.sub(r'&lt;!--.*?--&gt;', '', text_clean).strip()\n            # Skip reply markers\n            if re.search(r'ğŸ¤–.*Reply.*ğŸ¤–', text):\n                continue\n            # Skip headers that are mostly emoji/special chars (no alphanumeric content)\n            if not re.search(r'[a-zA-Z0-9]', text_clean):\n                continue\n            base_slug = re.sub(r'[^a-z0-9]+', '-', text_clean.lower()).strip('-')\n            slug_counts[base_slug] += 1\n            slug = base_slug if slug_counts[base_slug] == 1 else f\"{base_slug}-{slug_counts[base_slug]}\"\n            headers.append((level, text_clean, slug))\n    return headers\nThe key addition is the if re.search(r'ğŸ¤–.*Reply.*ğŸ¤–', text): continue block that skips any header containing the reply marker pattern.\nI was concerned that using SQLite would be slower than using static files to store blog posts. After thinking about it though, it seems that sqlite is a local file that we are reading. There is no network latency, and because sqlite is a single file, reads are super fast. Therefore there is no real difference in storing static markdown files vs storing their content in the sqlite db.",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "core.html#development",
    "href": "core.html#development",
    "title": "core",
    "section": "Development",
    "text": "Development\n\n# db.t.post.drop(ignore=True)\n\n\nserver = JupyUvi(app)\n\nGo to Solveit dashboard and click â€˜Copy Public URLâ€™ button to getâ€¦public URL to see any app running (by default port 8000)\n\nserver.stop()",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "core.html#system-design",
    "href": "core.html#system-design",
    "title": "core",
    "section": "System Design",
    "text": "System Design\nah there is a /data dir in my project root that pla.sh uses. Docs are here\nImportant to note - they do auto backups so you can restore if something goes haywire: https://docs.pla.sh/how_to/restore_backups.html\nWhat I eventually want: write a dialog and export via code to the /data dir (or do I? Im not even sure)\nI just want a more streamlined way to post blogs.\nRight now, the only way I can do this with minimal effort is to export to a markdown gist, then process the gist to extract the md content and put it in the db.\nNeed a backup solution at some point. Canâ€™t keep a production db within solveit that I risk overwriting (even though plash does have a restore option in worst case)\nok so lets say I use the /data folder in my project root to write blog posts. Id want to have a nbs dir within in for ipynb files and also a posts dir which contains the posts that I convert from ipynb to md. does this make sense?\n\nğŸ¤–ReplyğŸ¤–\nğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ \nYes, that makes sense! So your structure would be:\nproject_root/\nâ”œâ”€â”€ data/\nâ”‚   â”œâ”€â”€ dev.db\nâ”‚   â”œâ”€â”€ prod.db\nâ”‚   â”œâ”€â”€ nbs/          # Author notebooks here\nâ”‚   â”‚   â””â”€â”€ my-post.ipynb\nâ”‚   â””â”€â”€ posts/        # Converted markdown\nâ”‚       â””â”€â”€ my-post.md\nThe workflow would be: write in nbs/, convert to posts/, then load from posts/ into the sqlite db.\nA couple of questions to clarify your intent:\n\nWhy both posts/ and sqlite? Are you thinking the md files are the â€œsource of truthâ€ and you periodically sync them into the db? Or is the db the source of truth once imported?\nConversion tool â€” are you planning to use nbdev or jupyter nbconvert to go from ipynb â†’ md?\n\nits a fair question. sqlite allows for quick indexing and grouping based on categories?\n\n\nğŸ¤–ReplyğŸ¤–\nğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ \nExactly right! SQLite gives you:\n\nFast queries â€” filter by category, date ranges, search by title, etc.\nIndexing â€” add indexes on slug, categories, etc. for even faster lookups\nRelationships â€” if you ever want tags, comments, or other linked data\n\nSo the markdown files in posts/ would be your source of truth (easy to version control, edit in any text editor), and the sqlite db is your runtime cache for fast querying.\nThe workflow would be: 1. Write/edit in nbs/ as notebooks 2. Convert to markdown â†’ posts/ 3. Run a sync script that reads posts/*.md and upserts into sqlite\nDoes that match what youâ€™re thinking? And do you want the sync to be manual (run a command) or automatic (detect file changes)?",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "core.html#deploying",
    "href": "core.html#deploying",
    "title": "core",
    "section": "Deploying",
    "text": "Deploying\nUsing https://pla.sh/dashboard\nyou need to create a requirements.txt file based on the above packages. use your tools to get the current packages versions. generate this requirements.txt file in the parent folder of this file\n\n!pip show python-fasthtml fastlite monsterui fastcore fastlucide | grep -E \"^(Name|Version)\"\n\nName: python-fasthtml\nVersion: 0.12.36\nName: fastlite\nVersion: 0.2.2\nName: monsterui\nVersion: 1.0.35\nName: fastcore\nVersion: 1.9.2\nName: fastlucide\nVersion: 0.0.6",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "core.html#export",
    "href": "core.html#export",
    "title": "core",
    "section": "Export",
    "text": "Export",
    "crumbs": [
      "core"
    ]
  }
]